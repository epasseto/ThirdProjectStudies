{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37e2385",
   "metadata": {},
   "source": [
    "# Lesson 5\n",
    "\n",
    "## A/B Testing Case Study\n",
    "\n",
    "This lesson:\n",
    "\n",
    "- build a user funnel $\\rightarrow$ decide on metrics $\\rightarrow$ perform experiment sizing\n",
    "\n",
    "Perform inferential statistics on metrics:\n",
    "\n",
    "- invariant\n",
    "\n",
    "- evaluation\n",
    "\n",
    "Previous lessons, learned about components:\n",
    "\n",
    "- conceptual\n",
    "\n",
    "- statistical\n",
    "\n",
    "For in an experiment:\n",
    "\n",
    "- design\n",
    "\n",
    "- analyse\n",
    "\n",
    "### Scenario Description\n",
    "\n",
    "#### On Udacity text:\n",
    "\n",
    "\"Let's say that you're working for a fictional productivity software company that is looking for ways to increase the number of people who pay for their software. The way that the software is currently set up, users can download and use the software free of charge, for a 7-day trial. After the end of the trial, users are required to pay for a license to continue using the software.\n",
    "\n",
    "One idea that the company wants to try is to change the layout of the homepage to emphasize more prominently and higher up on the page that there is a 7-day trial available for the company's software. The current fear is that some potential users are missing out on using the software because of a lack of awareness of the trial period. If more people download the software and use it in the trial period, the hope is that this entices more people to make a purchase after seeing what the software can do.\n",
    "\n",
    "In this case study, you'll go through steps for planning out an experiment to test the new homepage. You will start by constructing a user funnel and deciding on metrics to track. You'll also perform experiment sizing to see how long it should be run. Afterwards, you'll be given some data collected for the experiment, perform statistical tests to analyze the results, and come to conclusions regarding how effective the new homepage changes were for bringing in more users.\"\n",
    "\n",
    "### Building a Funnel\n",
    "\n",
    "### Deciding on Metrics\n",
    "\n",
    "### Experiment Sizing\n",
    "\n",
    "### Validity, Bias, Ethics\n",
    "\n",
    "### Analyze Data\n",
    "\n",
    "### Draw Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429009c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lesson 6\n",
    "\n",
    "##  Recommendation Engines\n",
    "\n",
    "---\n",
    "\n",
    "## Movie Tweeting Data\n",
    "\n",
    "## First Notebook - L5 - Intro to Recommendation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9d73f",
   "metadata": {},
   "source": [
    "### Recommendations with MovieTweetings: Most Popular Recommendation\n",
    "\n",
    "#### On Udacity text:\n",
    "\n",
    "\"Now that you have created the necessary columns we will be using throughout the rest of the lesson on creating recommendations, let's get started with the first of our recommendations.\"\n",
    "\n",
    "To get started, read in the libraries and the two datasets you will be using throughout the lesson using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "import pickle\n",
    "import udacourse3\n",
    "\n",
    "import tests as t\n",
    "import helper as h\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3128a74",
   "metadata": {},
   "source": [
    "#### Part I: How To Find The Most Popular Movies?\n",
    "\n",
    "#### On Udacity text:\n",
    "\n",
    "\"For this notebook, we have a single task.  The task is that no matter the user, we need to provide a list of the recommendations based on simply the most popular items.\n",
    "\n",
    "For this task, we will consider what is \"most popular\" based on the following criteria:\n",
    "\n",
    "* A movie with the highest average rating is considered best\n",
    "* With ties, movies that have more ratings are better\n",
    "* A movie must have a minimum of 5 ratings to be considered among the best movies\n",
    "* If movies are tied in their average rating and number of ratings, the ranking is determined by the movie that is the most recent rating\"\n",
    "\n",
    "With these criteria, the goal for this notebook is to take a **user_id** and provide back the **num_top** recommendations.  Use the function below as the scaffolding that will be used for all the future recommendations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.groupby('movie_id')['rating'].mean().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be1440",
   "metadata": {},
   "source": [
    "function `fn_ranked_movie` created!\n",
    "\n",
    "function `fn_popular_recomendation` created!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb2636",
   "metadata": {},
   "source": [
    "#### On Udacity text:\n",
    "\n",
    "\"Using the three criteria above, you should be able to put together the above function.  If you feel confident in your solution, check the results of your function against our solution. On the next page, you can see a walkthrough and you can of course get the solution by looking at the solution notebook available in this workspace.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1\n",
    "ranked_movie = udacourse3.fn_create_ranked_df(movie, \n",
    "                                              review,\n",
    "                                              verbose=True) # only run this once - it is not fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_20_for_1 = udacourse3.fn_popular_recommendation(user_id='1', \n",
    "                                                     num_top=20, \n",
    "                                                     ranked_movie=ranked_movie,\n",
    "                                                     verbose=True)\n",
    "# Top 5 movies recommended for id 53968\n",
    "recs_5_for_53968 = udacourse3.fn_popular_recommendation(user_id='53968', \n",
    "                                                        num_top=5, \n",
    "                                                        ranked_movie=ranked_movie,\n",
    "                                                        verbose=True)\n",
    "# Top 100 movies recommended for id 70000\n",
    "recs_100_for_70000 = udacourse3.fn_popular_recommendation(user_id='70000', \n",
    "                                                          num_top=100, \n",
    "                                                          ranked_movie=ranked_movie,\n",
    "                                                          verbose=True)\n",
    "# Top 35 movies recommended for id 43\n",
    "recs_35_for_43 = udacourse3.fn_popular_recommendation(user_id='43', \n",
    "                                                      num_top=35, \n",
    "                                                      ranked_movie=ranked_movie,\n",
    "                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You Should Not Need To Modify Anything In This Cell\n",
    "# check 1 \n",
    "assert t.popular_recommendations('1', 20, ranked_movie) == recs_20_for_1,  \"The first check failed...\"\n",
    "# check 2\n",
    "assert t.popular_recommendations('53968', 5, ranked_movie) == recs_5_for_53968,  \"The second check failed...\"\n",
    "# check 3\n",
    "assert t.popular_recommendations('70000', 100, ranked_movie) == recs_100_for_70000,  \"The third check failed...\"\n",
    "# check 4\n",
    "assert t.popular_recommendations('43', 35, ranked_movie) == recs_35_for_43,  \"The fourth check failed...\"\n",
    "\n",
    "print(\"If you got here, looks like you are good to go!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26318ca",
   "metadata": {},
   "source": [
    "#### On Udacity text:\n",
    "\n",
    "Top rated $\\rightarrow$ is a fluid concept, and could depend on:\n",
    "\n",
    ">- trending news\n",
    ">- trending social events\n",
    ">- a time window\n",
    "\n",
    "**Notice:** \n",
    "\n",
    "\"This wasn't the only way we could have determined the \"top rated\" movies. You can imagine that in keeping track of trending news or trending social events, you would likely want to create a time window from the current time, and then pull the articles in the most recent time frame.  There are always going to be some subjective decisions to be made.  \n",
    "\n",
    "If you find that no one is paying any attention to your most popular recommendations, then it might be time to find a new way to recommend, which is what the next parts of the lesson should prepare us to do!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904fe9d",
   "metadata": {},
   "source": [
    "### Part II: Adding Filters\n",
    "\n",
    "#### On Udacity text:\n",
    "\n",
    "Filters can bring $\\rightarrow$ robustness for our model\n",
    "\n",
    "**Robustnes** (asking Google) has two meanings:\n",
    "\n",
    ">- \"the quality or condition of being strong and in good condition\"\n",
    ">- \"the ability to withstand or overcome adverse conditions or rigorous testing\"\n",
    "\n",
    "\"Now that you have created a function to give back the **num_top** movies, let's make it a bit more robust.  Add arguments that will act as filters for the movie **year** and **genre**.\" \n",
    "\n",
    "Use the cells below to adjust your existing function to allow for **year** and **genre** arguments as **lists** of **strings**.  Then your ending results are filtered to only movies within the lists of provided years and genres (as `or` conditions).  If no list is provided, there should be no filter applied.\n",
    "\n",
    "You can adjust other necessary inputs as necessary to retrieve the final results you are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b56e34",
   "metadata": {},
   "source": [
    "function `fn_popular_recommendation_filtered` created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b245bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1 with years=['2015', '2016', '2017', '2018'], genres=['History']\n",
    "recs_20_for_1_filtered = udacourse3.fn_popular_recommendation_filtered(user_id='1', \n",
    "                                                                       num_top=20, \n",
    "                                                                       ranked_movie=ranked_movie,\n",
    "                                                                       year=['2015', '2016', '2017', '2018'], \n",
    "                                                                       genre=['History'])\n",
    "\n",
    "# Top 5 movies recommended for id 53968 with no genre filter but years=['2015', '2016', '2017', '2018']\n",
    "recs_5_for_53968_filtered = udacourse3.fn_popular_recommendation_filtered(user_id='53968', \n",
    "                                                                          num_top=5, \n",
    "                                                                          ranked_movie=ranked_movie, \n",
    "                                                                          year=['2015', '2016', '2017', '2018'])\n",
    "\n",
    "# Top 100 movies recommended for id 70000 with no year filter but genres=['History', 'News']\n",
    "recs_100_for_70000_filtered = udacourse3.fn_popular_recommendation_filtered(user_id='70000', \n",
    "                                                                            num_top=100, \n",
    "                                                                            ranked_movie=ranked_movie, \n",
    "                                                                            genre=['History', 'News'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e043307",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You Should Not Need To Modify Anything In This Cell\n",
    "# check 1 \n",
    "assert t.popular_recs_filtered('1', 20, ranked_movie, years=['2015', '2016', '2017', '2018'], genres=['History']) == recs_20_for_1_filtered,  \"The first check failed...\"\n",
    "# check 2\n",
    "assert t.popular_recs_filtered('53968', 5, ranked_movie, years=['2015', '2016', '2017', '2018']) == recs_5_for_53968_filtered,  \"The second check failed...\"\n",
    "# check 3\n",
    "assert t.popular_recs_filtered('70000', 100, ranked_movie, genres=['History', 'News']) == recs_100_for_70000_filtered,  \"The third check failed...\"\n",
    "print(\"If you got here, looks like you are good to go!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f614d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ways to Reccomend - Knowledge Based\n",
    "\n",
    "## Second Notebook - L8 - Most Popular Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06c300",
   "metadata": {},
   "source": [
    "### Recommendations with MovieTweetings: Most Popular Recommendation\n",
    "\n",
    "Now that you have created the necessary columns we will be using throughout the rest of the lesson on creating recommendations, let's get started with the first of our recommendations.\n",
    "\n",
    "To get started, read in the libraries and the two datasets you will be using throughout the lesson using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db63f2",
   "metadata": {},
   "source": [
    "#### Part I: How To Find The Most Popular Movies?\n",
    "\n",
    "For this notebook, we have a single task.  The task is that no matter the user, we need to provide a list of the recommendations based on simply the most popular items.\n",
    "\n",
    "For this task, we will consider what is \"most popular\" based on the following criteria:\n",
    "\n",
    "* A movie with the highest average rating is considered best\n",
    "* With ties, movies that have more ratings are better\n",
    "* A movie must have a minimum of 5 ratings to be considered among the best movies\n",
    "* If movies are tied in their average rating and number of ratings, the ranking is determined by the movie that is the most recent rating\n",
    "\n",
    "With these criteria, the goal for this notebook is to take a **user_id** and provide back the **n_top** recommendations.  Use the function below as the scaffolding that will be used for all the future recommendations as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33c635",
   "metadata": {},
   "source": [
    "Using the three criteria above, you should be able to put together the above function.  If you feel confident in your solution, check the results of your function against our solution. On the next page, you can see a walkthrough and you can of course get the solution by looking at the solution notebook available in this workspace.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df2cba",
   "metadata": {},
   "source": [
    "Function `fn_create_ranked_movie` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this once - it is not fast\n",
    "ranked_movie = udacourse3.fn_create_ranked_df(movie, \n",
    "                                              review,\n",
    "                                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_movie.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146844e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1\n",
    "recs_20_for_1 = udacourse3.fn_popular_recommendation(\n",
    "    user_id='1', \n",
    "    num_top=20, \n",
    "    ranked_movie=ranked_movie,\n",
    "    verbose=True\n",
    ")\n",
    "# Top 5 movies recommended for id 53968\n",
    "recs_5_for_53968 = udacourse3.fn_popular_recommendation(\n",
    "    user_id='53968', \n",
    "    num_top=5, \n",
    "    ranked_movie=ranked_movie,\n",
    "    verbose=True\n",
    ")\n",
    "# Top 100 movies recommended for id 70000\n",
    "recs_100_for_70000 = udacourse3.fn_popular_recommendation(\n",
    "    user_id='70000', \n",
    "    num_top=100, \n",
    "    ranked_movie=ranked_movie,\n",
    "    verbose=True\n",
    ")\n",
    "# Top 35 movies recommended for id 43\n",
    "recs_35_for_43 = udacourse3.fn_popular_recommendation(\n",
    "    user_id='43', \n",
    "    num_top=35, \n",
    "    ranked_movie=ranked_movie,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You Should Not Need To Modify Anything In This Cell\n",
    "# check 1 \n",
    "assert t.popular_recommendations('1', 20, ranked_movie) == recs_20_for_1,  \"The first check failed...\"\n",
    "# check 2\n",
    "assert t.popular_recommendations('53968', 5, ranked_movie) == recs_5_for_53968,  \"The second check failed...\"\n",
    "# check 3\n",
    "assert t.popular_recommendations('70000', 100, ranked_movie) == recs_100_for_70000,  \"The third check failed...\"\n",
    "# check 4\n",
    "assert t.popular_recommendations('43', 35, ranked_movie) == recs_35_for_43,  \"The fourth check failed...\"\n",
    "print(\"If you got here, looks like you are good to go!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38629589",
   "metadata": {},
   "source": [
    "**Notice:** This wasn't the only way we could have determined the \"top rated\" movies.  You can imagine that in keeping track of trending news or trending social events, you would likely want to create a time window from the current time, and then pull the articles in the most recent time frame.  There are always going to be some subjective decisions to be made.  \n",
    "\n",
    "If you find that no one is paying any attention to your most popular recommendations, then it might be time to find a new way to recommend, which is what the next parts of the lesson should prepare us to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2051e1b",
   "metadata": {},
   "source": [
    "### Part II: Adding Filters\n",
    "\n",
    "Now that you have created a function to give back the **n_top** movies, let's make it a bit more robust.  Add arguments that will act as filters for the movie **year** and **genre**.  \n",
    "\n",
    "Use the cells below to adjust your existing function to allow for **year** and **genre** arguments as **lists** of **strings**.  Then your ending results are filtered to only movies within the lists of provided years and genres (as `or` conditions).  If no list is provided, there should be no filter applied.\n",
    "\n",
    "You can adjust other necessary inputs as necessary to retrieve the final results you are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1 with years=['2015', '2016', '2017', '2018'], genres=['History']\n",
    "recs_20_for_1_filtered = udacourse3.fn_popular_recommendation_filtered(\n",
    "    user_id='1', \n",
    "    num_top=20, \n",
    "    ranked_movie=ranked_movie,\n",
    "    year=['2015', '2016', '2017', '2018'], \n",
    "    genre=['History']\n",
    ")\n",
    "# Top 5 movies recommended for id 53968 with no genre filter but years=['2015', '2016', '2017', '2018']\n",
    "recs_5_for_53968_filtered = udacourse3.fn_popular_recommendation_filtered(\n",
    "    user_id='53968', \n",
    "    num_top=5, \n",
    "    ranked_movie=ranked_movie,\n",
    "    year=['2015', '2016', '2017', '2018']\n",
    ")\n",
    "# Top 100 movies recommended for id 70000 with no year filter but genres=['History', 'News']\n",
    "recs_100_for_70000_filtered = udacourse3.fn_popular_recommendation_filtered(\n",
    "    user_id='70000', \n",
    "    num_top=100, \n",
    "    ranked_movie=ranked_movie,\n",
    "    genre=['History', 'News']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55693da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You Should Not Need To Modify Anything In This Cell\n",
    "# check 1 \n",
    "assert t.popular_recs_filtered('1', \n",
    "                               20, \n",
    "                               ranked_movie, \n",
    "                               years=['2015', '2016', '2017', '2018'], \n",
    "                               genres=['History']) == recs_20_for_1_filtered,  \"The first check failed...\"\n",
    "# check 2\n",
    "assert t.popular_recs_filtered('53968', \n",
    "                               5, \n",
    "                               ranked_movie, \n",
    "                               years=['2015', '2016', '2017', '2018']) == recs_5_for_53968_filtered,\\\n",
    "\"The second check failed...\"\n",
    "# check 3\n",
    "assert t.popular_recs_filtered('70000', \n",
    "                               100, \n",
    "                               ranked_movie,\n",
    "                               genres=['History', 'News']) == recs_100_for_70000_filtered,\\\n",
    "\"The third check failed...\"\n",
    "print(\"If you got here, looks like you are good to go!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb829e40",
   "metadata": {},
   "source": [
    "## More Personalized Ways - Collaborative Filtering & Content Based\n",
    "\n",
    "## Third Notebook - L14 - Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380d35c",
   "metadata": {},
   "source": [
    "### How to Find Your Neighbor?\n",
    "\n",
    "As in k-Neighbors Classifier, some way to identify them $\\rightarrow$ similar subjects = similar preferences\n",
    "\n",
    "\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"In neighborhood based collaborative filtering, it is incredibly important to be able to identify an individual's neighbors.  Let's look at a small dataset in order to understand, how we can use different metrics to identify close neighbors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fde5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data = pd.DataFrame({'x1': [-3, -2, -1, 0, 1, 2, 3], \n",
    "               'x2': [9, 4, 1, 0, 1, 4, 9],\n",
    "               'x3': [1, 2, 3, 4, 5, 6, 7],\n",
    "               'x4': [2, 5, 15, 27, 28, 30, 31]\n",
    "})\n",
    "\n",
    "#create play data dataframe\n",
    "play_data = play_data[['x1', 'x2', 'x3', 'x4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4b437",
   "metadata": {},
   "source": [
    "### Measures of Similarity\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"The first metrics we will look at have similar characteristics:\n",
    "\n",
    "1. Pearson's Correlation Coefficient\n",
    "2. Spearman's Correlation Coefficient\n",
    "3. Kendall's Tau\"\n",
    "\n",
    "### Pearson's Correlation\n",
    "\n",
    "relation between data in **X-axis** and data in **Y-axis** [statquest](https://www.youtube.com/watch?v=xZ_z8KWkhXE&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "green apples vs red apples:\n",
    "\n",
    ">- normally I draw a line (don't matter the slope)\n",
    ">- looking for **weak** or **strong** relationship (correlation)\n",
    ">- [-1, 0] for **negative correlations** \n",
    "\n",
    "leads to... $R^2$ that can be **not linear**!\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"First, **Pearson's correlation coefficient** is a measure related to the strength and direction of a **linear** relationship.  \n",
    "\n",
    "If we have two vectors x and y, we can compare their individual elements in the following way to calculate Pearson's correlation coefficient:\n",
    "\n",
    "$$CORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y_i-\\bar{y})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i$$\n",
    "\"\n",
    "\n",
    "1. Write a function that takes in two vectors and returns the Pearson correlation coefficient.  You can then compare your answer to the built in function in numpy by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c03b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will test your function against the built in numpy function\n",
    "assert udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                         play_data['x2'],\n",
    "                                         corr_type='pearson') == np.corrcoef(play_data['x1'], \n",
    "                                                                             play_data['x2'])[0][1],\\\n",
    "'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'\\\n",
    ".format(udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                          play_data['x2'],\n",
    "                                          corr_type='pearson'))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                               play_data['x3'],\n",
    "                                               corr_type='pearson'), 2) == np.corrcoef(play_data['x1'], \n",
    "                                                                                       play_data['x3'])[0][1],\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], \n",
    "                                     udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                                                       play_data['x3'],\n",
    "                                                                       corr_type='pearson'))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x3'], \n",
    "                                               play_data['x4'],\n",
    "                                               corr_type='pearson'), 2) == round(np.corrcoef(play_data['x3'], \n",
    "                                                                                             play_data['x4'])[0][1], \n",
    "                                                                                 2),\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], \n",
    "                                     udacourse3.fn_compute_correlation(play_data['x3'], \n",
    "                                                                       play_data['x4'],\n",
    "                                                                       corr_type='pearson'))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Pearson's correlation coefficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da04341",
   "metadata": {},
   "source": [
    "`2.` Now that you have computed **Pearson's correlation coefficient**, use the below dictionary to identify statements that are true about **this** measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "pearson_dct = {\"If when x increases, y always increases, Pearson's correlation will be always be 1.\": b,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": b\n",
    "}\n",
    "\n",
    "t.sim_2_sol(pearson_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815c614",
   "metadata": {},
   "source": [
    "### Spearman's Correlation\n",
    "\n",
    "- Pearson vs Spearman [here](https://www.youtube.com/watch?v=c5ASFOYd918&ab_channel=StatistikinDD)\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"Now, let's look at **Spearman's correlation coefficient**.  Spearman's correlation is what is known as a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) statistic, which is a statistic who's distribution doesn't depend parameters (statistics that follow normal distributions or binomial distributions are examples of parametric statistics).  \n",
    "\n",
    "Frequently non-parametric statistics are based on the ranks of data rather than the original values collected.  This happens to be the case with Spearman's correlation coefficient, which is calculated similarly to Pearson's correlation.  However, instead of using the raw data, we use the rank of each value.\"\n",
    "\n",
    "You can quickly change from the raw data to the ranks using the **.rank()** method as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ranked values for the variable x1 are: {}\".format(np.array(play_data['x1'].rank())))\n",
    "print(\"The raw data values for the variable x1 are: {}\".format(np.array(play_data['x1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d092a9d",
   "metadata": {},
   "source": [
    "#### In Udacity text:\n",
    "\n",
    "\"If we map each of our data to ranked data values as shown above:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Here, we let the **r** indicate these are ranked values (this is not raising any value to the power of r).  Then we compute Spearman's correlation coefficient as:\n",
    "\n",
    "$$SCORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x^{r}_i - \\bar{x}^{r})(y^{r}_i - \\bar{y}^{r})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x^{r}_i-\\bar{x}^{r})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y^{r}_i-\\bar{y}^{r})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x}^r = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x^r_i$$\n",
    "\n",
    "`3.` Write a function that takes in two vectors and returns the Spearman correlation coefficient.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576d598",
   "metadata": {},
   "source": [
    "function `fn_compute_correlation` created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                         play_data['x2'],\n",
    "                                         corr_type='spearman') == spearmanr(play_data['x1'], \n",
    "                                                                            play_data['x2'])[0],\\\n",
    "'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'\\\n",
    ".format(compute_corr(play_data['x1'], play_data['x2']))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                               play_data['x3'],\n",
    "                                               corr_type='spearman'), 2) == spearmanr(play_data['x1'], \n",
    "                                                                                      play_data['x3'])[0],\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], compute_corr(play_data['x1'], play_data['x3']))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x3'], \n",
    "                                               play_data['x4'],\n",
    "                                               corr_type='spearman'), 2) == round(spearmanr(play_data['x3'], \n",
    "                                                                                            play_data['x4'])[0], 2),\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], compute_corr(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Spearman's correlation coefficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45446d",
   "metadata": {},
   "source": [
    "`4.` Now that you have computed **Spearman's correlation coefficient**, use the below dictionary to identify statements that are true about **this** measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "spearman_dct = {\"If when x increases, y always increases, Spearman's correlation will be always be 1.\": a,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": a\n",
    "}\n",
    "\n",
    "t.sim_4_sol(spearman_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0af1d",
   "metadata": {},
   "source": [
    "### Kendall's Tau\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Kendall's tau is quite similar to Spearman's correlation coefficient.  Both of these measures are nonparametric measures of a relationship.  Specifically both Spearman and Kendall's coefficients are calculated based on ranking data and not the raw data.  \n",
    "\n",
    "Similar to both of the previous measures, Kendall's Tau is always between -1 and 1, where -1 suggests a strong, negative relationship between two variables and 1 suggests a strong, positive relationship between two variables.\n",
    "\n",
    "Though Spearman's and Kendall's measures are very similar, there are statistical advantages to choosing Kendall's measure in that Kendall's Tau has smaller variability when using larger sample sizes.  However Spearman's measure is more computationally efficient, as Kendall's Tau is O(n^2) and Spearman's correlation is O(nLog(n)). You can find more on this topic in [this thread](https://www.researchgate.net/post/Does_Spearmans_rho_have_any_advantage_over_Kendalls_tau).\n",
    "\n",
    "Let's take a closer look at exactly how this measure is calculated.  Again, we want to map our data to ranks:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Then we calculate Kendall's Tau as:\n",
    "\n",
    "$$TAU(\\textbf{x}, \\textbf{y}) = \\frac{2}{n(n -1)}\\sum_{i < j}sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "Where $sgn$ takes the the sign associated with the difference in the ranked values.  An alternative way to write \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)$$ \n",
    "\n",
    "is in the following way:\n",
    "\n",
    "$$\n",
    " \\begin{cases} \n",
    "      -1  & x^r_i < x^r_j \\\\\n",
    "      0 & x^r_i = x^r_j \\\\\n",
    "      1 & x^r_i > x^r_j \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore the possible results of \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "are only 1, -1, or 0, which are summed to give an idea of the propotion of times the ranks of **x** and **y** are pointed in the right direction.\"\n",
    "\n",
    "#### Task\n",
    "\n",
    "`5.` Write a function that takes in two vectors and returns Kendall's Tau.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f492d5",
   "metadata": {},
   "source": [
    "function `fn_compute_correlation` improved for Kendall Tau!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                         play_data['x2'],\n",
    "                                         corr_type='kendall_tau') == kendalltau(play_data['x1'], \n",
    "                                                                        play_data['x2'])[0],\\\n",
    "'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'\\\n",
    ".format(udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                          play_data['x2'],\n",
    "                                          type='kendall_tau'))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                               play_data['x3'],\n",
    "                                               corr_type='kendall_tau'), 2) == kendalltau(play_data['x1'], \n",
    "                                                                                     play_data['x3'])[0],\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(kendalltau(play_data['x1'], \n",
    "                   play_data['x3'])[0][1], udacourse3.fn_compute_correlation(play_data['x1'], \n",
    "                                                                             play_data['x3'],\n",
    "                                                                             corr_type='kendall_tau'))\n",
    "assert round(udacourse3.fn_compute_correlation(play_data['x3'], \n",
    "                                               play_data['x4'],\n",
    "                                               corr_type='kendall_tau'), 2) == round(kendalltau(play_data['x3'], \n",
    "                                                                                                play_data['x4'])[0], \n",
    "                                                                                     2),\\\n",
    "'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'\\\n",
    ".format(kendalltau(play_data['x3'], play_data['x4'])[0][1], udacourse3.fn_compute_correlation(play_data['x3'],\n",
    "                                                                                              play_data['x4'],\n",
    "                                                                                              corr_type='kendall_tau'))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Kendall's Tau!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c24e6",
   "metadata": {},
   "source": [
    "`6.` Use your functions (and/or your knowledge of each of the above coefficients) to accurately identify each of the below statements as True or False.  **Note:** There may be some rounding differences due to the way numbers are stored, so it is recommended that you consider comparisons to 4 or fewer decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c39882",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "corr_comp_dct = {\"For all columns of play_data, Spearman and Kendall's measures match.\": a,\n",
    "                 \"For all columns of play_data, Spearman and Pearson's measures match.\": b, \n",
    "                 \"For all columns of play_data, Pearson and Kendall's measures match.\": b}\n",
    "\n",
    "t.sim_6_sol(corr_comp_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea045b",
   "metadata": {},
   "source": [
    "### Distance Measures\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Each of the above measures are considered measures of correlation.  Similarly, there are distance measures (of which there are many).  [This is a great article](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/) on some popular distance metrics.  In this notebook, we will be looking specifically at two of these measures.  \n",
    "\n",
    "1. Euclidean Distance\n",
    "2. Manhattan Distance\n",
    "\n",
    "Different than the three measures you built functions for, these two measures take on values between 0 and potentially infinity.  Measures that are closer to 0 imply that two vectors are more similar to one another.  The larger these values become, the more dissimilar two vectors are to one another.\n",
    "\n",
    "Choosing one of these two `distance` metrics vs. one of the three `similarity` above is often a matter of personal preference, audience, and data specificities.  You will see in a bit a case where one of these measures (euclidean or manhattan distance) is optimal to using Pearson's correlation coefficient.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Euclidean distance can also just be considered as straight-line distance between two vectors.\n",
    "\n",
    "For two vectors **x** and **y**, we can compute this as:\n",
    "\n",
    "$$ EUC(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "\"\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Different from euclidean distance, Manhattan distance is a 'manhattan block' distance from one vector to another.  Therefore, you can imagine this distance as a way to compute the distance between two points when you are not able to go through buildings.\n",
    "\n",
    "Specifically, this distance is computed as:\n",
    "\n",
    "$$ MANHATTAN(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}|x_i - y_i|}$$\n",
    "\n",
    "Using each of the above, write a function for each to take two vectors and compute the euclidean and manhattan distances.\n",
    "\n",
    "\n",
    "![distances](graphs/distances.png)\n",
    "\n",
    "You can see in the above image, the **blue** line gives the **Manhattan** distance, while the **green** line gives the **Euclidean** distance between two points.\"\n",
    "\n",
    "#### Task\n",
    "\n",
    "`7.` Use the below cell to complete a function for each distance metric.  Then test your functions against the built in values using the below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65641bfa",
   "metadata": {},
   "source": [
    "function `fn_calculate_distance` created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your functions\n",
    "assert h.test_eucl(play_data['x1'], play_data['x2']) == udacourse3.fn_calculate_distance(play_data['x1'], \n",
    "                                                                                         play_data['x2'],\n",
    "                                                                                         dist_type='euclidean')\n",
    "assert h.test_eucl(play_data['x2'], play_data['x3']) == udacourse3.fn_calculate_distance(play_data['x2'], \n",
    "                                                                                         play_data['x3'],\n",
    "                                                                                         dist_type='euclidean')\n",
    "assert h.test_manhat(play_data['x1'], play_data['x2']) == udacourse3.fn_calculate_distance(play_data['x1'], \n",
    "                                                                                           play_data['x2'],\n",
    "                                                                                           dist_type='manhattan')\n",
    "assert h.test_manhat(play_data['x2'], play_data['x3']) == udacourse3.fn_calculate_distance(play_data['x2'], \n",
    "                                                                                           play_data['x3'],\n",
    "                                                                                           dist_type='manhattan')\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e895a50",
   "metadata": {},
   "source": [
    "### Final Note\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"It is worth noting that two vectors could be similar by metrics like the three at the top of the notebook, while being incredibly, incredibly different by measures like these final two.  Again, understanding your specific situation will assist in understanding whether your metric is appropriate.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afd8bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Identifying Reccomendations\n",
    "\n",
    "## Forth Notebook - L17 - Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176290a",
   "metadata": {},
   "source": [
    "## Recommendations with MovieTweetings: Collaborative Filtering\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"One of the most popular methods for making recommendations is **collaborative filtering**.  In collaborative filtering, you are using the collaboration of user-item recommendations to assist in making new recommendations.  \n",
    "\n",
    "There are two main methods of performing collaborative filtering:\n",
    "\n",
    "1. **Neighborhood-Based Collaborative Filtering**, which is based on the idea that we can either correlate items that are similar to provide recommendations or we can correlate users to one another to provide recommendations.\n",
    "\n",
    "2. **Model Based Collaborative Filtering**, which is based on the idea that we can use machine learning and other mathematical models to understand the relationships that exist amongst items and users to predict ratings and provide ratings.\n",
    "\n",
    "\n",
    "In this notebook, you will be working on performing **neighborhood-based collaborative filtering**.  There are two main methods for performing collaborative filtering:\n",
    "\n",
    "1. **User-based collaborative filtering:** In this type of recommendation, users related to the user you would like to make recommendations for are used to create a recommendation.\n",
    "\n",
    "2. **Item-based collaborative filtering:** In this type of recommendation, first you need to find the items that are most related to each other item (based on similar ratings).  Then you can use the ratings of an individual on those similar items to understand if a user will like the new item.\"\n",
    "\n",
    "In this notebook you will be implementing **user-based collaborative filtering**.  However, it is easy to extend this approach to make recommendations using **item-based collaborative filtering**.  First, let's read in our data and necessary libraries.\n",
    "\n",
    "**NOTE**: Because of the size of the datasets, some of your code cells here will take a while to execute, so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfcf6c",
   "metadata": {},
   "source": [
    "### Measures of Similarity\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"When using **neighborhood** based collaborative filtering, it is important to understand how to measure the similarity of users or items to one another.  \n",
    "\n",
    "There are a number of ways in which we might measure the similarity between two vectors (which might be two users or two items).\"  \n",
    "\n",
    "In this notebook, we will look specifically at two measures used to compare vectors:\n",
    "\n",
    "* **Pearson's correlation coefficient**\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Pearson's correlation coefficient is a measure of the strength and direction of a linear relationship. The value for this coefficient is a value between -1 and 1 where -1 indicates a strong, negative linear relationship and 1 indicates a strong, positive linear relationship. \n",
    "\n",
    "If we have two vectors x and y, we can define the correlation between the vectors as:\n",
    "\n",
    "\n",
    "$$CORR(x, y) = \\frac{\\text{COV}(x, y)}{\\text{STDEV}(x)\\text{ }\\text{STDEV}(y)}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{STDEV}(x) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\text{COV}(x, y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "where n is the length of the vector, which must be the same for both x and y and $\\bar{x}$ is the mean of the observations in the vector.  \n",
    "\n",
    "We can use the correlation coefficient to indicate how alike two vectors are to one another, where the closer to 1 the coefficient, the more alike the vectors are to one another.  There are some potential downsides to using this metric as a measure of similarity.  You will see some of these throughout this workbook.\"\n",
    "\n",
    "\n",
    "* **Euclidean distance**\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Euclidean distance is a measure of the straightline distance from one vector to another.  Because this is a measure of distance, larger values are an indication that two vectors are different from one another (which is different than Pearson's correlation coefficient).\n",
    "\n",
    "Specifically, the euclidean distance between two vectors x and y is measured as:\n",
    "\n",
    "$$ \\text{EUCL}(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "Different from the correlation coefficient, no scaling is performed in the denominator.  Therefore, you need to make sure all of your data are on the same scale when using this metric.\n",
    "\n",
    "**Note:** Because measuring similarity is often based on looking at the distance between vectors, it is important in these cases to scale your data or to have all data be in the same scale.  In this case, we will not need to scale data because they are all on a 10 point scale, but it is always something to keep in mind!\"\n",
    "\n",
    "------------\n",
    "\n",
    "### User-Item Matrix\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"In order to calculate the similarities, it is common to put values in a matrix.  In this matrix, users are identified by each row, and items are represented by columns.\"\n",
    "\n",
    "![user x item](graphs/userxitem.png \"User Item Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6ea54",
   "metadata": {},
   "source": [
    "#### In Udacity notes:\n",
    "\n",
    "\"In the above matrix, you can see that **User 1** and **User 2** both used **Item 1**, and **User 2**, **User 3**, and **User 4** all used **Item 2**.  However, there are also a large number of missing values in the matrix for users who haven't used a particular item.  A matrix with many missing values (like the one above) is considered **sparse**.\"\n",
    "\n",
    "---\n",
    "\n",
    "Our first goal for this notebook is to create the above matrix with the **reviews** dataset.  However, instead of 1 values in each cell, you should have the actual rating.  \n",
    "\n",
    "The users will indicate the rows, and the movies will exist across the columns. To create the user-item matrix, we only need the first three columns of the **reviews** dataframe, which you can see by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item = review[['user_id', 'movie_id', 'rating']]\n",
    "user_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e42fa",
   "metadata": {},
   "source": [
    "### Creating the User-Item Matrix\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"In order to create the user-items matrix (like the one above), I personally started by using a [pivot table](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html). \n",
    "\n",
    "However, I quickly ran into a memory error (a common theme throughout this notebook).  I will help you navigate around many of the errors I had, and achieve useful collaborative filtering results!\"\n",
    "\n",
    "_____\n",
    "\n",
    "`1.` Create a matrix where the users are the rows, the movies are the columns, and the ratings exist in each cell, or a NaN exists in cells where a user hasn't rated a particular movie. If you get a memory error (like I did), [this link here](https://stackoverflow.com/questions/39648991/pandas-dataframe-pivot-memory-error) might help you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d244d0",
   "metadata": {},
   "source": [
    "function `udacourse3.fn_create_user_item` created!\n",
    "\n",
    "renamed to `udacourse3.fn_create_user_movie`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20887f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_movie = udacourse3.fn_create_user_movie(df_user_item=user_item, \n",
    "                                                verbose=True)\n",
    "user_by_movie.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8332be2",
   "metadata": {},
   "source": [
    "Check your results below to make sure your matrix is ready for the upcoming sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert movie.shape[0] == user_by_movie.shape[1],\\\n",
    "\"Oh no! Your matrix should have {} columns, and yours has {}!\".format(movie.shape[0], user_by_movie.shape[1])\n",
    "assert review.user_id.nunique() == user_by_movie.shape[0],\\\n",
    "\"Oh no! Your matrix should have {} rows, and yours has {}!\".format(review.user_id.nunique(), user_by_movie.shape[0])\n",
    "print(\"Looks like you are all set! Proceed!\")\n",
    "#HTML('<img src=\"graphs/greatjob.webp\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e319f",
   "metadata": {},
   "source": [
    "`2.` Now that you have a matrix of users by movies, use this matrix to create a dictionary where the key is each user and the value is an array of the movies each user has rated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74697cbd",
   "metadata": {},
   "source": [
    "function `fn_movie_watched` created!\n",
    "\n",
    "- iterate over `user_by_movie` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72949c",
   "metadata": {},
   "source": [
    "#### Note: this is big data processing!\n",
    "    \n",
    "So, the first time you run this notebook, you need to uncomment the following lines for creating the file `watched.pkl` in your computer. Then turn to comment these lines, for just loading the data, saving processing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1fcb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#watched = udacourse3.fn_movie_watched(df_user_movie=user_by_movie,\n",
    "#                                      user_id=66,\n",
    "#                                      lower_filter=None,\n",
    "#                                      verbose=True)\n",
    "#watched[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57acf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('watched.pkl', 'wb') as handle:\n",
    "#    pickle.dump(watched, handle)\n",
    "\n",
    "with open('watched.pkl', 'rb') as handle:\n",
    "    watched = pickle.load(handle)\n",
    "\n",
    "watched[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2bd01",
   "metadata": {},
   "source": [
    "function `fn_create_movie_dict` created!\n",
    "\n",
    "- iterate over `user_by_movie` dataset\n",
    "\n",
    "- this is a polimorphic function, so you can enter as `df_user_movie` an already created dictionnary, or a Pandas dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e531c50",
   "metadata": {},
   "source": [
    "#### Note: this is big data processing!\n",
    "    \n",
    "So, the first time you run this notebook, you need to uncomment the following lines for creating the file `seen.pkl` in your computer. Then turn to comment these lines, for just loading the data, saving processing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_seen = udacourse3.fn_create_user_movie_dict(df_user_movie=user_by_movie,\n",
    "#                                                   lower_filter=None,\n",
    "#                                                   verbose=False)\n",
    "#len(movie_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('seen.pkl', 'wb') as handle:\n",
    "#    pickle.dump(movie_seen, handle)\n",
    "\n",
    "with open('seen.pkl', 'rb') as handle:\n",
    "    movie_seen = pickle.load(handle)\n",
    "    \n",
    "len(movie_seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23da40",
   "metadata": {},
   "source": [
    "`3.` If a user hasn't rated more than 2 movies, we consider these users \"too new\".  Create a new dictionary that only contains users who have rated more than 2 movies.  This dictionary will be used for all the final steps of this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as a dataset\n",
    "#movie_filtered = udacourse3.fn_create_user_movie_dict(df_user_movie=user_by_movie,\n",
    "#                                                      lower_filter=2,\n",
    "#                                                      verbose=True)\n",
    "#len(movies_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88450404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using our already created dictionnary\n",
    "movie_to_analyze = udacourse3.fn_create_user_movie_dict(df_user_movie=movie_seen,\n",
    "                                                        lower_filter=2,\n",
    "                                                        verbose=True)\n",
    "#for usr in movies_to_analyze.keys():\n",
    "#    print(movies_to_analyze[usr])\n",
    "len(movie_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7236475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests below to check that your movies_to_analyze matches the solution\n",
    "assert len(movie_to_analyze) == 23512,\\\n",
    "\"Oops!  It doesn't look like your dictionary has the right number of individuals.\"\n",
    "assert len(movie_to_analyze[2]) == 23,\\\n",
    "\"Oops!  User 2 didn't match the number of movies we thought they would have.\"\n",
    "assert len(movie_to_analyze[7])  == 3,\\\n",
    "\"Oops!  User 7 didn't match the number of movies we thought they would have.\"\n",
    "print(\"If this is all you see, you are good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433164a",
   "metadata": {},
   "source": [
    "### Calculating User Similarities\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Now that you have set up the **movies_to_analyze** dictionary, it is time to take a closer look at the similarities between users. Below is the pseudocode for how I thought about determining the similarity between users:\n",
    "\n",
    "```\n",
    "for user1 in movies_to_analyze\n",
    "    for user2 in movies_to_analyze\n",
    "        see how many movies match between the two users\n",
    "        if more than two movies in common\n",
    "            pull the overlapping movies\n",
    "            compute the distance/similarity metric between ratings on the same movies for the two users\n",
    "            store the users and the distance metric\n",
    "```\n",
    "\n",
    "However, this took a very long time to run, and other methods of performing these operations did not fit on the workspace memory!\n",
    "\n",
    "Therefore, rather than creating a dataframe with all possible pairings of users in our data, your task for this question is to look at a few specific examples of the correlation between ratings given by two users.  For this question consider you want to compute the [correlation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) between users.\"\n",
    "\n",
    "`4.` Using the **movies_to_analyze** dictionary and **user_by_movie** dataframe, create a function that computes the correlation between the ratings of similar movies for two users.  Then use your function to compare your results to ours using the tests below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274403e",
   "metadata": {},
   "source": [
    "function `fn_take_correlation` created!\n",
    "\n",
    "- iterate over `user_by_movie` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = 2\n",
    "user2 = 66\n",
    "correlation = udacourse3.fn_take_correlation(for_user1=user_by_movie.loc[user1], \n",
    "                                             for_user2=user_by_movie.loc[user2],\n",
    "                                             verbose=True)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7443dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the solution\n",
    "assert udacourse3.fn_take_correlation(for_user1=user_by_movie.loc[2],\n",
    "                                      for_user2=user_by_movie.loc[2]) == 1.0,\\\n",
    "\"Oops!  The correlation between a user and itself should be 1.0.\"\n",
    "assert round(udacourse3.fn_take_correlation(for_user1=user_by_movie.loc[2],\n",
    "                                            for_user2=user_by_movie.loc[66]), 2) == 0.76,\\\n",
    "\"Oops!  The correlation between user 2 and 66 should be about 0.76.\"\n",
    "assert np.isnan(udacourse3.fn_take_correlation(for_user1=user_by_movie.loc[2],\n",
    "                                               for_user2=user_by_movie.loc[104])),\\\n",
    "\"Oops!  The correlation between user 2 and 104 should be a NaN.\"\n",
    "print(\"If this is all you see, then it looks like your function passed all of our tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bffe7",
   "metadata": {},
   "source": [
    "### Why the NaN's?\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"If the function you wrote passed all of the tests, then you have correctly set up your function to calculate the correlation between any two users.\"  \n",
    "\n",
    "`5.` But one question is, why are we still obtaining **NaN** values?  As you can see in the code cell above, users 2 and 104 have a correlation of **NaN**. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da146fab",
   "metadata": {},
   "source": [
    "#### In Udacity notes:\n",
    "\n",
    "\"Think and write your ideas here about why these NaNs exist, and use the cells below to do some coding to validate your thoughts. You can check other pairs of users and see that there are actually many NaNs in our data - 2,526,710 of them in fact. **These NaN's ultimately make the correlation coefficient a less than optimal measure of similarity between two users.**\n",
    "\n",
    "```\n",
    "In the denominator of the correlation coefficient, we calculate the standard deviation for each user's ratings.  The ratings for user 2 are all the same rating on the movies that match with user 104.  Therefore, the standard deviation is 0.  Because a 0 is in the denominator of the correlation coefficient, we end up with a **NaN** correlation coefficient.  Therefore, a different approach is likely better for this particular situation.\n",
    "```\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b668236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which movies did both user 2 and user 104 see?\n",
    "set_2 = set(movie_to_analyze[2])\n",
    "set_104 = set(movie_to_analyze[104])\n",
    "set_2.intersection(set_104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the ratings for each user on those movies?\n",
    "print(user_by_movie.loc[2, set_2.intersection(set_104)])\n",
    "print(user_by_movie.loc[104, set_2.intersection(set_104)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460f8c",
   "metadata": {},
   "source": [
    "`6.` Because the correlation coefficient proved to be less than optimal for relating user ratings to one another, we could instead calculate the euclidean distance between the ratings.  I found [this post](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) particularly helpful when I was setting up my function.  This function should be very similar to your previous function.  When you feel confident with your function, test it against our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_dist(user1, user2):\n",
    "    movies1 = movie_to_analyze[user1]\n",
    "    movies2 = movie_to_analyze[user2]\n",
    "    sim_movs = np.intersect1d(movies1, movies2, assume_unique=True)\n",
    "    df = user_by_movie.loc[(user1, user2), sim_movs] #not necessary\n",
    "    dist = np.linalg.norm(df.loc[user1] - df.loc[user2])\n",
    "    return (sim_movs, df, df.loc[user1], df.loc[user2], dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtup = compute_euclidean_dist(user1=2, user2=66)\n",
    "print('euclidean distance:', rtup[4])\n",
    "print('identical movies id:',rtup[0])\n",
    "print('series for user1:', rtup[2])\n",
    "print('series for user2:', rtup[3])\n",
    "rtup[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7e51d",
   "metadata": {},
   "source": [
    "function `fn_take_euclidean_dist` created!\n",
    "\n",
    "- iterate over `user_by_movie` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = udacourse3.fn_take_euclidean_dist(for_user1=user_by_movie.loc[2], \n",
    "                                              for_user2=user_by_movie.loc[66],\n",
    "                                              verbose=True)\n",
    "euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in solution euclidean distances\"\n",
    "df_dist = pd.read_pickle(\"data/dists.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the solution\n",
    "assert udacourse3.fn_take_euclidean_dist(\n",
    "    for_user1=user_by_movie.loc[2],\n",
    "    for_user2=user_by_movie.loc[2]) == df_dist.query(\"user1 == 2 and user2 == 2\")['eucl_dist'][0],\\\n",
    "\"Oops!  The distance between a user and itself should be 0.0.\"\n",
    "assert round(udacourse3.fn_take_euclidean_dist(\n",
    "    for_user1=user_by_movie.loc[2],\n",
    "    for_user2=user_by_movie.loc[66]),2) == round(df_dist.query(\"user1 == 2 and user2 == 66\")['eucl_dist'][1], 2),\\\n",
    "\"Oops!  The distance between user 2 and 66 should be about 2.24.\"\n",
    "assert np.isnan(udacourse3.fn_take_euclidean_dist(\n",
    "    for_user1=user_by_movie.loc[2],\n",
    "    for_user2=user_by_movie.loc[66])) == np.isnan(df_dist.query(\"user1 == 2 and user2 == 104\")['eucl_dist'][4]),\\\n",
    "\"Oops!  The distance between user 2 and 104 should be 2.\"\n",
    "print(\"If this is all you see, then it looks like your function passed all of our tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f52bdf",
   "metadata": {},
   "source": [
    "### Using the Nearest Neighbors to Make Recommendations\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"In the previous question, you read in **df_dists**. Therefore, you have a measure of distance between each user and every other user. This dataframe holds every possible pairing of users, as well as the corresponding euclidean distance.\n",
    "\n",
    "Because of the **NaN** values that exist within the correlations of the matching ratings for many pairs of users, as we discussed above, we will proceed using **df_dists**. You will want to find the users that are 'nearest' each user.  Then you will want to find the movies the closest neighbors have liked to recommend to each user.\n",
    "\n",
    "I made use of the following objects:\n",
    "\n",
    "* df_dists (to obtain the neighbors)\n",
    "* user_items (to obtain the movies the neighbors and users have rated)\n",
    "* movies (to obtain the names of the movies)\"\n",
    "\n",
    "`7.` Complete the functions below, which allow you to find the recommendations for any user.  There are five functions which you will need:\n",
    "\n",
    "* **find_closest_neighbors** - this returns a list of user_ids from closest neighbor to farthest neighbor using euclidean distance\n",
    "\n",
    "* **movies_liked** - returns an array of movie_ids\n",
    "\n",
    "\n",
    "* **movie_names** - takes the output of movies_liked and returns a list of movie names associated with the movie_ids\n",
    "\n",
    "\n",
    "* **make_recommendations** - takes a user id and goes through closest neighbors to return a list of movie names as recommendations\n",
    "\n",
    "\n",
    "* **all_recommendations** = loops through every user and returns a dictionary of with the key as a user_id and the value as a list of movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 2\n",
    "#df_dists.head(10)\n",
    "filt_user1 = df_dist[df_dist['user1'] == user]\n",
    "filt_user1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = filt_user1['user1'].iloc[0]\n",
    "filt_user2 = filt_user1[filt_user1['user2'] != user1]\n",
    "filt_user2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e253030",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = 2\n",
    "closest_user = df_dist[df_dist['user1']==user1].sort_values(by='eucl_dist').iloc[1:]#['user2']\n",
    "closest_user.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_neighbor = np.array(closest_user)\n",
    "closest_neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_user2.sort_values(by='eucl_dist')[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166ba10",
   "metadata": {},
   "source": [
    "function `fn_find_closest_neighbor` created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 2\n",
    "neighbor = udacourse3.fn_find_closest_neighbor(filt_user1=df_dist[df_dist['user1'] == user],\n",
    "                                               limit=10,\n",
    "                                               verbose=True)\n",
    "for i in range (1,5):\n",
    "    print(neighbor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 66\n",
    "movie_liked = user_item[(user_item['user_id'] == user) & (user_item['rating'] > 7)]\n",
    "movie_liked.sort_values(by='rating').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5be836",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_liked.iloc[0]['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a00bf2",
   "metadata": {},
   "source": [
    "filter with variable value:\n",
    "\n",
    "    \n",
    "`.query(user_id == @user_id and rating > (@min_rating -1)['movie_id'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f035b5",
   "metadata": {},
   "source": [
    "function `fn_movie_liked` created!\n",
    "\n",
    "function `fn_movie_liked2` created!\n",
    "\n",
    "- iterate over `user_by_movie` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 66\n",
    "user_item[user_item['user_id'] == user_id].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42439162",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item[user_item['user_id'] == user_id]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4927cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_movie.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = user_by_movie.loc[user_id].dropna()\n",
    "data2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2 > 7].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated function!\n",
    "#user_id = 66\n",
    "#liked = udacourse3.fn_movie_liked(item=user_item[user_item['user_id'] == user_id],\n",
    "#                                  verbose=True)\n",
    "#liked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c730957",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 66\n",
    "liked = udacourse3.fn_movie_liked2(item=user_by_movie.loc[user_id].dropna(),\n",
    "                                   sort=True,\n",
    "                                   verbose=True)\n",
    "#liked[0]\n",
    "len(liked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie[movie['movie_id'].isin(liked)].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a68e1",
   "metadata": {},
   "source": [
    "Original filtering machine:\n",
    "\n",
    "`movies[movies['movie_id'].isin(movie_ids)]['movie']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_retrieved = udacourse3.fn_movie_name(df_movie=movie,\n",
    "                                           movie_id=liked,\n",
    "                                           verbose=True)\n",
    "movie_retrieved[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "watched = udacourse3.fn_movie_watched(df_user_movie=user_by_movie,\n",
    "                                      user_id=66,\n",
    "                                      lower_filter=None,\n",
    "                                      verbose=True)\n",
    "watched[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_movie.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_id = 33854\n",
    "\n",
    "filt_user = user_by_movie.loc[user_id].dropna()\n",
    "filt_user.index\n",
    "\n",
    "item=filt_user\n",
    "\n",
    "movie_liked = item[item > 7]\n",
    "movie_liked\n",
    "np.array(movie_liked.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist[df_dist['user1'] == user].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe55db",
   "metadata": {},
   "source": [
    "function `fn_make_recommendation` created!\n",
    "\n",
    "name altered to `fn_make_recommendation_collab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49096425",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=66\n",
    "udacourse3.fn_make_recommendation_collab(filt_dist=df_dist[df_dist['user1'] == user],\n",
    "                                         df_user_movie=user_by_movie,\n",
    "                                         df_movie=movie,\n",
    "                                         num_rec=10,\n",
    "                                         limit=100,\n",
    "                                         min_rating=7,\n",
    "                                         sort=True,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2446855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist[df_dist['user1'] == user].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199157f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 66\n",
    "isinstance(user_id, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_movie.loc[user_id].dropna().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594c11e",
   "metadata": {},
   "source": [
    "function `fn_all_recommendation` created!\n",
    "\n",
    "renamed to `fn_all_recommendation_collab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d01bb",
   "metadata": {},
   "source": [
    "#### Note: this is big data processing!\n",
    "    \n",
    "So, the first time you run this notebook, you need to uncomment the following lines for creating the file `recommended.pkl` in your computer. Then turn to comment these lines, for just loading the data, saving processing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4591984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_recs = udacourse3.fn_all_recommendation_collab(\n",
    "#               df_dist=df_dist,\n",
    "#               df_user_movie=user_by_movie,\n",
    "#               df_movie=movie,\n",
    "#               num_rec=10,\n",
    "#               limit=100,\n",
    "#               min_rating=7,\n",
    "#               sort=False,                                 \n",
    "#               verbose=False)\n",
    "#len(all_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('recommended.pkl', 'wb') as handle:\n",
    "#    pickle.dump(all_recs, handle)\n",
    "\n",
    "with open('recommended.pkl', 'rb') as handle:\n",
    "    all_recs = pickle.load(handle)\n",
    "    \n",
    "len(all_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d96473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads our solution dictionary so you can compare results\n",
    "#FULL PATH IS \"data/Term2/recommendations/lesson1/data/all_recs.p\"\n",
    "all_recs_sol = pd.read_pickle(\"data/all_recs.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_recs[2] == udacourse3.fn_make_recommendation_collab(\n",
    "                          filt_dist=df_dist[df_dist['user1'] == 2],\n",
    "                          df_user_movie=user_by_movie,\n",
    "                          df_movie=movie),\\\n",
    "\"Oops!  Your recommendations for user 2 didn't match ours.\"\n",
    "assert all_recs[26] == udacourse3.fn_make_recommendation_collab(\n",
    "                          filt_dist=df_dist[df_dist['user1'] == 26],\n",
    "                          df_user_movie=user_by_movie,\n",
    "                          df_movie=movie),\\\n",
    "\"Oops!  It actually wasn't possible to make any recommendations for user 26.\"\n",
    "assert all_recs[1503] == udacourse3.fn_make_recommendation_collab(\n",
    "                          filt_dist=df_dist[df_dist['user1'] == 1503],\n",
    "                          df_user_movie=user_by_movie,\n",
    "                          df_movie=movie),\\\n",
    "\"Oops! Looks like your solution for user 1503 didn't match ours.\"\n",
    "print(\"If you made it here, you now have recommendations for many users using collaborative filtering!\")\n",
    "#HTML('<img src=\"images/greatjob.webp\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16672afa",
   "metadata": {},
   "source": [
    "### Now What?\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"If you made it this far, you have successfully implemented a solution to making recommendations using collaborative filtering.\"\n",
    "\n",
    "`8.` Let's do a quick recap of the steps taken to obtain recommendations using collaborative filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd59a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your understanding of the results by correctly filling in the dictionary below\n",
    "a = \"pearson's correlation and spearman's correlation\"\n",
    "b = 'item based collaborative filtering'\n",
    "c = \"there were too many ratings to get a stable metric\"\n",
    "d = 'user based collaborative filtering'\n",
    "e = \"euclidean distance and pearson's correlation coefficient\"\n",
    "f = \"manhattan distance and euclidean distance\"\n",
    "g = \"spearman's correlation and euclidean distance\"\n",
    "h = \"the spread in some ratings was zero\"\n",
    "i = 'content based recommendation'\n",
    "\n",
    "sol_dict = {\n",
    "    'The type of recommendation system implemented here was a ...': d,\n",
    "    'The two methods used to estimate user similarity were: ': e,\n",
    "    'There was an issue with using the correlation coefficient.  What was it?': h\n",
    "}\n",
    "\n",
    "t.test_recs(sol_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86b67",
   "metadata": {},
   "source": [
    "\"Additionally, let's take a closer look at some of the results.  There are two solution files that you read in to check your results, and you created these objects\n",
    "\n",
    "* **df_dists** - a dataframe of user1, user2, euclidean distance between the two users\n",
    "* **all_recs_sol** - a dictionary of all recommendations (key = user, value = list of recommendations)\" \n",
    "\n",
    "`9.` Use these two objects along with the cells below to correctly fill in the dictionary below and complete this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f60542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload \n",
    "#import tests as t\n",
    "#t = reload(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 567\n",
    "b = 1503\n",
    "c = 1319\n",
    "d = 1325\n",
    "e = 2526710\n",
    "f = 0\n",
    "g = 'Use another method to make recommendations - content based, knowledge based, model based collaborative filtering'\n",
    "\n",
    "sol_dict2 = {\n",
    "    'For how many pairs of users were we not able to obtain a measure of similarity using correlation?': e,\n",
    "    'For how many pairs of users were we not able to obtain a measure of similarity using euclidean distance?': f,\n",
    "    'For how many users were we unable to make any recommendations for using collaborative filtering?': c,\n",
    "    'For how many users were we unable to make 10 recommendations for using collaborative filtering?': d,\n",
    "    'What might be a way for us to get 10 recommendations for every user?': g   \n",
    "}\n",
    "\n",
    "t.test_recs2(sol_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users without recs\n",
    "users_without_recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) == 0:\n",
    "        users_without_recs.append(user)\n",
    "    \n",
    "len(users_without_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN euclidean distance values\n",
    "df_dist['eucl_dist'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users with fewer than 10 recs\n",
    "users_with_less_than_10recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) < 10:\n",
    "        users_with_less_than_10recs.append(user)\n",
    "    \n",
    "len(users_with_less_than_10recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15f3b5",
   "metadata": {},
   "source": [
    "## Ways to Reccomend - Content Based\n",
    "\n",
    "## Fifth Notebook - L 21 - Content Based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b2b1",
   "metadata": {},
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"In the previous notebook, you were introduced to a way to make recommendations using collaborative filtering.  However, using this technique there are a large number of users who were left without any recommendations at all.  Other users were left with fewer than the ten recommendations that were set up by our function to retrieve...\"\n",
    "\n",
    "In order to help these users out, let's try another technique **content based** recommendations.  Let's start off where we were in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)\n",
    "\n",
    "all_rec = pickle.load(open(\"data/all_recs.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ad723",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"From the above, you now have access to three important items that you will be using throughout the rest of this notebook.  \n",
    "\n",
    "`a.` **movie** - a dataframe of all of the movies in the dataset along with other content related information about the movies (genre and date)\n",
    "\n",
    "\n",
    "`b.` **review** - this was the main dataframe used before for collaborative filtering, as it contains all of the interactions between users and movies.\n",
    "\n",
    "\n",
    "`c.` **all_rec** - a dictionary where each key is a user, and the value is a list of movie recommendations based on collaborative filtering\n",
    "\n",
    "For the individuals in **all_rec** who did recieve 10 recommendations using collaborative filtering, we don't really need to worry about them.  However, there were a number of individuals in our dataset who did not receive any recommendations.\"\n",
    "\n",
    "-----\n",
    "\n",
    "`1.` Let's start with finding all of the users in our dataset who didn't get all 10 ratings we would have liked them to have using collaborative filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ce958",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_with_all_rec = []\n",
    "for user, movie_rec in all_rec.items():\n",
    "    if len(movie_rec) > 9:\n",
    "        user_with_all_rec.append(user)\n",
    "\n",
    "print(\"There are {} users with all reccomendations from collaborative filtering.\".format(len(user_with_all_rec)))\n",
    "\n",
    "user = np.unique(review['user_id'])\n",
    "user_who_need_rec = np.setdiff1d(user, user_with_all_rec)\n",
    "\n",
    "print(\"There are {} users who still need recommendations.\".format(len(user_who_need_rec)))\n",
    "print(\"This means that only {}% of users received all 10 of their recommendations using collaborative filtering\"\\\n",
    "      .format(round(len(user_with_all_rec)/len(np.unique(review['user_id'])), 4)*100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce821a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some test here might be nice\n",
    "assert len(user_with_all_rec) == 22187\n",
    "print(\"That's right there were still another 31781 users who needed recommendations \\\n",
    "when we only used collaborative filtering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac9cca",
   "metadata": {},
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"You will be doing a bit of a mix of content and collaborative filtering to make recommendations for the users this time.  This will allow you to obtain recommendations in many cases where we didn't make recommendations earlier.\"\n",
    "\n",
    "`2.` Before finding recommendations, rank the user's ratings from highest to lowest. You will move through the movies in this order looking for other similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe similar to reviews, but ranked by rating for each user\n",
    "ranked_review = review.sort_values(by=['user_id', 'rating'], \n",
    "                                   ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c77f6",
   "metadata": {},
   "source": [
    "### Similarities\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"In the collaborative filtering sections, you became quite familiar with different methods of determining the similarity (or distance) of two users.  We can perform similarities based on content in much the same way.  \n",
    "\n",
    "In many cases, it turns out that one of the fastest ways we can find out how similar items are to one another (when our matrix isn't totally sparse like it was in the earlier section) is by simply using matrix multiplication.  If you are not familiar with this, an explanation is available [here by 3blue1brown](https://www.youtube.com/watch?v=LyGKycYT2v0) and another quick explanation is provided [on the post here](https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity).\n",
    "\n",
    "For us to pull out a matrix that describes the movies in our dataframe in terms of content, we might just use the indicator variables related to **year** and **genre** for our movies.  \n",
    "\n",
    "Then we can obtain a matrix of how similar movies are to one another by taking the dot product of this matrix with itself.  Notice in the below that the dot product where our 1 values overlap gives a value of 2 indicating higher similarity.  In the second dot product, the 1 values don't match up.  This leads to a dot product of 0 indicating lower similarity.\n",
    "\n",
    "<img src=\"graphs/dotprod1.png\" alt=\"Dot Product\" height=\"500\" width=\"500\">\n",
    "\n",
    "We can perform the dot product on a matrix of movies with content characteristics to provide a movie by movie matrix where each cell is an indication of how similar two movies are to one another.  In the below image, you can see that movies 1 and 8 are most similar, movies 2 and 8 are most similar and movies 3 and 9 are most similar for this subset of the data.  The diagonal elements of the matrix will contain the similarity of a movie with itself, which will be the largest possible similarity (which will also be the number of 1's in the movie row within the orginal movie content matrix.\n",
    "\n",
    "<img src=\"graphs/moviemat.png\" alt=\"Dot Product\" height=\"500\" width=\"500\">\n",
    "\n",
    "\"\n",
    "\n",
    "`3.` Create a numpy array that is a matrix of indicator variables related to year (by century) and movie genres by movie.  Perform the dot product of this matrix with itself (transposed) to obtain a similarity matrix of each movie with every other movie.  The final matrix should be 31245 x 31245."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f47f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.iloc[:,4:].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cad9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movie.iloc[:,4:])\n",
    "movie_content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fcd9b",
   "metadata": {},
   "source": [
    "#### Note: this is big data processing!\n",
    "    \n",
    "So, the first time you run this notebook, you need to uncomment the following lines for creating the file `dot_prod.pkl` in your computer. Then turn to comment these lines, for just loading the data, saving processing time!\n",
    "\n",
    "Take the dot product to obtain a movie x movie matrix of similarities:\n",
    "\n",
    "*Observation: I could not save the `dot_prod.pkl` in my computer. So every time that I need it, I need to run the following lines...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a54496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "begin = time()\n",
    "\n",
    "dot_prod_movie = movie_content.dot(np.transpose(movie_content))\n",
    "\n",
    "end = time()\n",
    "print('elapsed time: {:.4f}s'.format(end-begin))\n",
    "dot_prod_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('dot_prod.pkl', 'wb') as handle:\n",
    "#    pickle.dump(dot_prod_movie, handle)\n",
    "\n",
    "#with open('dot_prod.pkl', 'rb') as handle:\n",
    "#    dot_prod_movie = pickle.load(handle)\n",
    "    \n",
    "#dot_prod_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04150e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checks for the dot product matrix\n",
    "assert dot_prod_movie.shape[0] == 31245\n",
    "assert dot_prod_movie.shape[1] == 31245\n",
    "assert dot_prod_movie[0, 0] == np.max(dot_prod_movie[0])\n",
    "print(\"Looks like you passed all of the tests.\")\n",
    "print(\"Though they weren't very robust - if you want to write some of your own, I won't complain!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e072e",
   "metadata": {},
   "source": [
    "### For Each User...\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Now that you have a matrix where each user has their ratings ordered.  You also have a second matrix where movies are each axis, and the matrix entries are larger where the two movies are more similar and smaller where the two movies are dissimilar.  This matrix is a measure of content similarity. Therefore, it is time to get to the fun part.\n",
    "\n",
    "For each user, we will perform the following:\n",
    "\n",
    "    i. For each movie, find the movies that are most similar that the user hasn't seen.\n",
    "\n",
    "    ii. Continue through the available, rated movies until 10 recommendations or until there are no additional movies.\n",
    "\n",
    "As a final note, you may need to adjust the criteria for 'most similar' to obtain 10 recommendations.  As a first pass, I used only movies with the highest possible similarity to one another as similar enough to add as a recommendation.\"\n",
    "\n",
    "`3.` In the below cell, complete each of the functions needed for making content based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_movie(movie_id):\n",
    "    movie_idx = np.where(movie['movie_id'] == movie_id)[0][0]\n",
    "    similar_idx = np.where(dot_prod_movie[movie_idx] == np.max(dot_prod_movie[movie_idx]))[0]\n",
    "    similar_movie = np.array(movie.iloc[similar_idx, ]['movie'])    \n",
    "    return similar_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71adb6",
   "metadata": {},
   "source": [
    "function `fn_find_similar_movie` created!\n",
    "\n",
    "function `fn_find_similar_movie` adapted to use `fn_get_movie_name` service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = udacourse3.fn_find_similar_movie(df_dot_product=dot_prod_movie,\n",
    "                                           df_movie=movie,\n",
    "                                           movie_id=2106284,\n",
    "                                           verbose=True)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bec21",
   "metadata": {},
   "source": [
    "function `fn_get_movie_name` created!\n",
    "\n",
    "- test for default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130086b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = [2106284, 231122344441]\n",
    "movie_id = 2106284\n",
    "udacourse3.fn_get_movie_name(df_movie=movie,\n",
    "                             movie_id=movie_id,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb569a",
   "metadata": {},
   "source": [
    "- test for `fn_find_similar_movie` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ae1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_idx = 21310\n",
    "udacourse3.fn_get_movie_name(df_movie=movie,\n",
    "                             movie_id=movie_idx,\n",
    "                             by_id=False,\n",
    "                             as_list=False,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215c129",
   "metadata": {},
   "source": [
    "function `fn_make_recommendation_content` created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a938c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=[2, 22]\n",
    "user=2\n",
    "rec = fn_make_recommendation_content(\n",
    "          df_dot_product=dot_prod_movie,\n",
    "          df_movie=movie,\n",
    "          user=user,\n",
    "          verbose=True)\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb78f6",
   "metadata": {},
   "source": [
    "### How Did We Do?\n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Now that you have made the recommendations, how did we do in providing everyone with a set of recommendations?\"\n",
    "\n",
    "`4.` Use the cells below to see how many individuals you were able to make recommendations for, as well as explore characteristics about individuals who you were not able to make recommendations for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e557694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore recommendations\n",
    "user_without_all_rec = []\n",
    "user_with_all_rec = []\n",
    "no_rec = []\n",
    "for user, movie_rec in rec.items():\n",
    "    if len(movie_rec) < 10:\n",
    "        user_without_all_rec.append(user)\n",
    "    if len(movie_rec) > 9:\n",
    "        user_with_all_rec.append(user)\n",
    "    if len(movie_rec) == 0:\n",
    "        no_rec.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf42db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some characteristics of my content based recommendations\n",
    "print(\"There were {} users without all 10 recommendations we would have liked to have.\"\\\n",
    "      .format(len(user_without_all_rec)))\n",
    "print(\"There were {} users with all 10 recommendations we would like them to have.\"\\\n",
    "      .format(len(user_with_all_rec)))\n",
    "print(\"There were {} users with no recommendations at all!\".format(len(no_rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import udacourse3\n",
    "\n",
    "udacourse3 = reload(udacourse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a closer look at individual user characteristics\n",
    "user_item = review[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = udacourse3.fn_create_user_movie(df_user_item=user_item, \n",
    "                                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_movie.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05de5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 189\n",
    "watched = udacourse3.fn_movie_watched(\n",
    "              df_user_movie=user_by_movie,\n",
    "              user_id=user_id,\n",
    "              verbose=True)\n",
    "watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330784a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "print(\"Some of the movie lists for users without any recommendations include:\")\n",
    "for user_id in no_rec:\n",
    "    print('user id:', user_id)\n",
    "    print(udacourse3.fn_get_movie_name(\n",
    "              df_movie=movie,\n",
    "              movie_id=fn_movie_watched(\n",
    "                  df_user_movie=user_by_movie,\n",
    "                  user_id=user_id,\n",
    "                  verbose=True)),\n",
    "              verbose=True)\n",
    "    counter += 1\n",
    "    if counter > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b9f3b",
   "metadata": {},
   "source": [
    "### Now What?  \n",
    "\n",
    "#### In Udacity notes:\n",
    "\n",
    "\"Well, if you were really strict with your criteria for how similar two movies are (like I was initially), then you still have some users that don't have all 10 recommendations (and a small group of users who have no recommendations at all). \n",
    "\n",
    "As stated earlier, recommendation engines are a bit of an **art** and a **science**.  There are a number of things we still could look into - how do our collaborative filtering and content based recommendations compare to one another? How could we incorporate user input along with collaborative filtering and/or content based recommendations to improve any of our recommendations?  How can we truly gain recommendations for every user?\"\n",
    "\n",
    "`5.` In this last step feel free to explore any last ideas you have with the recommendation techniques we have looked at so far.  You might choose to make the final needed recommendations using the first technique with just top ranked movies.  You might also loosen up the strictness in the similarity needed between movies.  Be creative and share your insights with your classmates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
