{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dea6114",
   "metadata": {},
   "source": [
    "# Lesson 7\n",
    "\n",
    "# Matrix Factorization for Recommendations\n",
    "\n",
    "## Questions still opened\n",
    "\n",
    "## How to know that our recommendation is a good one?\n",
    "\n",
    "#### Training and Testing Data For Recommendations\n",
    "\n",
    "**prior** to implement our recommendation $\\rightarrow$ look for a metric of our interest\n",
    "\n",
    "metric x time\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"In the last lesson, you were making recommendations by providing a list of popular items, or a list of items that the user hadn't observed but that someone with similar tastes had observed. However, understanding if these recommendations are good in practice means that you have to deploy these recommendations to users and see how it impacts your metrics (sales, higher engagement, clicks, conversions, etc.).\n",
    "\n",
    "You may not want your recommendations to go live to understand how well they work. In these cases, you will want to split your data into training and testing portions. In these cases, you can train your recommendation engine on a subset of the data, then you can test how well your recommendation engine performs on a test set of data before deploying your model to the world.\n",
    "\n",
    "However, the cases you saw in the last lesson, where just a list of recommendations was provided, don't actually lend themselves very well to training and testing methods of evaluation. In the next upcoming pages, you will be introduced to matrix factorization, which actually does work quite well for these situations.\"\n",
    "\n",
    "### Why to use SVD?\n",
    "\n",
    "#### Singular Value Decomposition\n",
    "\n",
    "Simon Funk $\\rightarrow$ FunkSVD\n",
    "\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"Singular Value Decomposition - If we let AA be our user-item matrix, we can write the decomposition of that matrix in the following way:\"\n",
    "\n",
    "$A = U \\Sigma V^T$\n",
    "\n",
    "Is a **Matrix Factorization** + **no** missing values (positive or negative)\n",
    "\n",
    "\n",
    "$U$ $\\rightarrow$ User ($n$) $\\times$ Latent Factor ($k$) $\\leftarrow$ how users are sensible to latent factors\n",
    "\n",
    "$V^T$ $\\rightarrow$ Latent Factor ($k$) $\\times$ Movie ($m$) $\\leftarrow$ how movies can provide some latent factors\n",
    "\n",
    "$\\Sigma$ is a $k \\times k$ largest to smallest diagonal matrix $\\leftarrow$ how much each latent factor weights to predict the rating (some were dropped down, as we restrict the size of $k$)\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"In the next part of this lesson, you will first get exposure to Singular Value Decomposition, or SVD. We will soon see why this technique falls short for many recommendation problems. However, understanding traditional SVD approaches to matrix factorization is useful as a start to a number of matrix factorization techniques that are possible in practice.\n",
    "\n",
    "In order to implement SVD for many recommendation engines, we will need to use a slightly modified approach known as FunkSVD. This approach proved to work incredibly well during the Netflix competition, and therefore, it is one of the most popular recommendation approaches in use today.\"\n",
    "\n",
    "\n",
    "#### Latent Factors\n",
    "\n",
    "is **not** a feature observed in your data (a movie is about robot-love, or kinky-sadism) $\\rightarrow$ there is a trigger between user-movie\n",
    "\n",
    "#### In Udacity text:\n",
    "\n",
    "\"When performing SVD, we create a matrix of users by items (or customers by movies in our specific example), with user ratings for each item scattered throughout the matrix. An example is shown in the image below.\n",
    "\n",
    "![latent factors](graphs\\latent.png)\n",
    "\n",
    "You can see that this matrix doesn't have any specific information about the users or items. Rather, it just holds the ratings that each user gave to each item. Using SVD on this matrix, we can find latent features related to the movies and customers. This is amazing because the dataset doesn't contain any information about the customers or movies!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b2e84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to use Machine Learning to make recommendations?\n",
    "\n",
    "split data in Train (**fit**) x Test (**evaluate**)\n",
    "\n",
    "#### Validating Your Recommendations\n",
    "\n",
    "old data $\\rightarrow$ fit\n",
    "\n",
    "new data $\\rightarrow$ evaluate\n",
    "\n",
    "**SVD** techniques can provide a **rating** for every user x item pair (you predicted an **8** and it was an **8**)\n",
    "\n",
    "Metrics lie **MSE** (Mean Squared Error) or **MAE** (Mean Absolute Error)\n",
    "\n",
    "#### Online Testing\n",
    "\n",
    "#### In Udacity text:\n",
    "\"For online methods of testing a recommender's performance, many of the methods you saw in the previous lesson work very well - you can deploy your recommendations and just watch your metrics carefully. It is common in practice to set up online recommendations to have an \"old\" version of recommended items, which is compared to a new page that uses a new recommendation strategy.\n",
    "\n",
    "All ideas associated with A/B testing that you learned in the earlier lessons are critical to watching your metrics in online learning, and ultimately, choosing a recommendation strategy that works best for your products and customers.\"\n",
    "\n",
    "#### Offline Testing\n",
    "\n",
    "#### In Udacity text:\n",
    "\"In many cases, a company might not let you simply deploy your recommendations out into the real world any time you feel like it. Testing out your recommendations in a training-testing environment prior to deploying them is called offline testing.\n",
    "\n",
    "The recommendation methods you built in the previous lesson actually don't work very well for offline testing. In offline testing, it is ideal to not just obtain a list of recommendations for each individual, because we ultimately don't know if a user doesn't use an item because they don't like it, or because they just haven't used it yet (but would like it). Rather, it would be great if we have an idea of how much each user would like each item using a predicted rating. Then we can compare this predicted rating to the actual rating any individual gives to an item in the future.\n",
    "\n",
    "In the previous video, you saw an example of a user to whom we gave a list of movies that they still hadn't seen. Therefore, we couldn't tell how well we were doing with our recommendations. Techniques related to matrix factorization lend themselves nicely to solving this problem.\"\n",
    "\n",
    "#### User Groups\n",
    "\n",
    "#### In Udacity text:\n",
    "\"The final (possible) method of validating your recommendations is by having user groups give feedback on items you would recommend for them. Obtaining good user groups that are representative of your customers can be a challenge on its own. This is especially true when you have a lot of products and a very large consumer base.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20b0c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to make recommendations to individuals that are new in the platform?\n",
    "\n",
    "the **Cold Start Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c06b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intro to SVD \n",
    "\n",
    "## First Notebook - L10 - Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ec1fd",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "So far in this lesson, you have gained some exposure to Singular Value Decomposition.  In this notebook, you will get some hands on practice with this technique.\n",
    "\n",
    "Let's get started by reading in our libraries and setting up the data we will be using throughout this notebook\n",
    "\n",
    "`1.` Run the cell below to create the **user_movie_subset** dataframe.  This will be the dataframe you will be using for the first part of this notebook.\n",
    "\n",
    "**Note: Unstacking the matrix here could take ~10 mins to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e9d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import svd_tests as t\n",
    "\n",
    "from time import time\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f501f85",
   "metadata": {},
   "source": [
    "You need to reallocate the maximum pagination (at Advanced Options on Windows) to at least **16Gb** and restart your computer before running this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3798f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43b4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id  73486  75314  68646  99685\n",
      "user_id                             \n",
      "265        10.0   10.0   10.0   10.0\n",
      "1023       10.0    4.0    9.0   10.0\n",
      "1683        8.0    9.0   10.0    5.0\n",
      "6571        9.0    8.0   10.0   10.0\n",
      "11639      10.0    5.0    9.0    9.0\n",
      "13006       6.0    4.0   10.0    6.0\n",
      "14076       9.0    8.0   10.0    9.0\n",
      "14725      10.0    5.0    9.0    8.0\n",
      "23548       7.0    8.0   10.0    8.0\n",
      "24760       9.0    5.0    9.0    7.0\n",
      "28713       9.0    8.0   10.0    8.0\n",
      "30685       9.0   10.0   10.0    9.0\n",
      "34110      10.0    9.0   10.0    8.0\n",
      "34430       5.0    8.0    5.0    8.0\n",
      "35150      10.0    8.0   10.0   10.0\n",
      "43294       9.0    9.0   10.0   10.0\n",
      "46849       9.0    8.0    8.0    8.0\n",
      "50556      10.0    8.0    1.0   10.0\n",
      "51382       5.0    6.0   10.0   10.0\n",
      "51410       8.0    7.0   10.0    7.0\n"
     ]
    }
   ],
   "source": [
    "# Create user-by-item matrix\n",
    "user_item = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "print(user_movie_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918327cc",
   "metadata": {},
   "source": [
    "`2.` Now that you have the **user_movie_subset** matrix, use this matrix to correctly match each key to the correct value in the dictionary below.  Use the cells below the dictionary as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15da25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  There are 20 users in the dataset, which is given by the number of rows. There are 4 movies in the dataset given by the number of columns.  You can find the movies or users with the highest average ratings by taking the mean of each row or column.  Using the movies table, you can find the movie names associated with each id.  This shows the top rated movie is The Godfather!\n"
     ]
    }
   ],
   "source": [
    "# match each letter to the best statement in the dictionary below - each will be used at most once\n",
    "a = 20\n",
    "b = 68646\n",
    "c = 'The Godfather'\n",
    "d = 'Goodfellas'\n",
    "e = 265\n",
    "f = 30685\n",
    "g = 4\n",
    "\n",
    "sol_1_dict = {\n",
    "    'the number of users in the user_movie_subset': a,\n",
    "    'the number of movies in the user_movie_subset': g,\n",
    "    'the user_id with the highest average ratings given': e,\n",
    "    'the movie_id with the highest average ratings received': b,\n",
    "    'the name of the movie that received the highest average rating': c\n",
    "}\n",
    "\n",
    "#test dictionary here\n",
    "t.test1(sol_1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97293fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "265      10.00\n",
      "1023      8.25\n",
      "1683      8.00\n",
      "6571      9.25\n",
      "11639     8.25\n",
      "13006     6.50\n",
      "14076     9.00\n",
      "14725     8.00\n",
      "23548     8.25\n",
      "24760     7.50\n",
      "28713     8.75\n",
      "30685     9.50\n",
      "34110     9.25\n",
      "34430     6.50\n",
      "35150     9.50\n",
      "43294     9.50\n",
      "46849     8.25\n",
      "50556     7.25\n",
      "51382     7.75\n",
      "51410     8.00\n",
      "dtype: float64\n",
      "movie_id\n",
      "73486    8.60\n",
      "75314    7.35\n",
      "68646    9.00\n",
      "99685    8.50\n",
      "dtype: float64\n",
      "4187    One Flew Over the Cuckoo's Nest (1975)\n",
      "Name: movie, dtype: object\n",
      "4361    Taxi Driver (1976)\n",
      "Name: movie, dtype: object\n",
      "3706    The Godfather (1972)\n",
      "Name: movie, dtype: object\n",
      "6917    Goodfellas (1990)\n",
      "Name: movie, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for work\n",
    "# user with the highest average rating\n",
    "print(user_movie_subset.mean(axis=1))\n",
    "\n",
    "# movie with highest average rating\n",
    "print(user_movie_subset.mean(axis=0))\n",
    "\n",
    "# list of movie names\n",
    "for movie_id in [73486, 75314,  68646, 99685]:\n",
    "    print(movie[movie['movie_id'] == movie_id]['movie'])\n",
    "    \n",
    "# users by movies\n",
    "user_movie_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba59e0f",
   "metadata": {},
   "source": [
    "ow that you have a little more context about the matrix we will be performing Singular Value Decomposition on, we're going to do just that.  To get started, let's remind ourselves about the dimensions of each of the matrices we are going to get back.   Essentially, we are going to split the **user_movie_subset** matrix into three matrices:\n",
    "\n",
    "$$ U \\Sigma V^T $$\n",
    "\n",
    "\n",
    "`3.` Given what you learned about in the previous parts of this lesson, provide the dimensions for each of the matrices specified above using the dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aae1c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  We will now put this to use, so you can see how the dot product of these matrices come together to create our user item matrix.  The number of latent features will control the sigma matrix as well, and this will a square matrix that will at most be the minimum of the number of users and number of movies (in our case the minimum is the 4 movies).\n"
     ]
    }
   ],
   "source": [
    "# match each letter in the dictionary below - a letter may appear more than once.\n",
    "a = 'a number that you can choose as the number of latent features to keep'\n",
    "b = 'the number of users'\n",
    "c = 'the number of movies'\n",
    "d = 'the sum of the number of users and movies'\n",
    "e = 'the product of the number of users and movies'\n",
    "\n",
    "sol_2_dict = {\n",
    "    'the number of rows in the U matrix': b, \n",
    "    'the number of columns in the U matrix': a, \n",
    "    'the number of rows in the V transpose matrix': a, \n",
    "    'the number of columns in the V transpose matrix': c\n",
    "}\n",
    "\n",
    "#test dictionary here\n",
    "t.test2(sol_2_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df1374",
   "metadata": {},
   "source": [
    "Now let's verify the above dimensions by performing SVD on our user-movie matrix.\n",
    "\n",
    "`4.` Below you can find the code used to perform SVD in numpy.  You can see more about this functionality in the [documentation here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html).  What do you notice about the shapes of your matrices?  If you try to take the dot product of the three objects you get back, can you directly do this to get back the user-movie matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ca1b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (20, 20), (4, 4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = np.linalg.svd(user_movie_subset)\n",
    "s.shape, u.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c356074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the dimensions of the three returned objects, we can see the following:\n",
      "\n",
      " 1. The u matrix is a square matrix with the number of rows and columns equaling the number of users. \n",
      "\n",
      " 2. The v transpose matrix is also a square matrix with the number of rows and columns equaling the number of items.\n",
      "\n",
      " 3. The sigma matrix is actually returned as just an array with 4 values, but should be a diagonal matrix.  Numpy has a diag method to help with this.  \n",
      "\n",
      " In order to set up the matrices in a way that they can be multiplied together, we have a few steps to perform: \n",
      "\n",
      " 1. Turn sigma into a square matrix with the number of latent features we would like to keep. \n",
      "\n",
      " 2. Change the columns of u and the rows of v transpose to match this number of dimensions. \n",
      "\n",
      " If we would like to exactly re-create the user-movie matrix, we could choose to keep all of the latent features.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell for our thoughts on the questions posted above\n",
    "t.question4thoughts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efc79e",
   "metadata": {},
   "source": [
    "`5.` Use the thoughts from the above question to create u, s, and vt with four latent features.  When you have all three matrices created correctly, run the test below to show that the dot product of the three matrices creates the original user-movie matrix.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (4 for this case)\n",
    "3. m is the number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430d3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "u_new = u[:, :len(s)]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_new = np.zeros((len(s), len(s)))\n",
    "s_new[:len(s), :len(s)] = np.diag(s) \n",
    "\n",
    "# Because we are using 4 latent features and there are only 4 movies, \n",
    "# vt and vt_new are the same\n",
    "vt_new = vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f645755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4.  The dot product of the three matrices how equals the original user-movie matrix!\n"
     ]
    }
   ],
   "source": [
    "# Check your matrices against the solution\n",
    "assert u_new.shape == (20, 4),\\\n",
    "\"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 4.\"\n",
    "assert s_new.shape == (4, 4),\\\n",
    "\"Oops!  The shape of the sigma matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert vt_new.shape == (4, 4),\\\n",
    "\"Oops! The shape of the v transpose matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert np.allclose(np.dot(np.dot(u_new, s_new), vt_new), user_movie_subset),\\\n",
    "\"Oops!  Something went wrong with the dot product.  Your result didn't reproduce the original movie_user matrix.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4\")\n",
    "print(\"The dot product of the three matrices how equals the original user-movie matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c87ed9",
   "metadata": {},
   "source": [
    "It turns out that the sigma matrix can actually tell us how much of the original variability in the user-movie matrix is captured by each latent feature.  The total amount of variability to be explained is the sum of the squared diagonal elements.  The amount of variability explained by the first componenet is the square of the first value in the diagonal.  The amount of variability explained by the second componenet is the square of the second value in the diagonal.   \n",
    "\n",
    "`6.` Using the above information, can you determine the amount of variability in the original user-movie matrix that can be explained by only using the first two components? Use the cell below for your work, and then test your answer against the solution with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a1e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total variance in the original matrix is 5877.0.\n",
      "Ther percentage of variability captured by the first two components is 98.55%.\n"
     ]
    }
   ],
   "source": [
    "total_var = np.sum(s**2)\n",
    "var_exp_comp1_and_comp2 = s[0]**2 + s[1]**2\n",
    "perc_exp = round(var_exp_comp1_and_comp2/total_var*100, 2)\n",
    "print(\"The total variance in the original matrix is {}.\".format(total_var))\n",
    "print(\"Ther percentage of variability captured by the first two components is {}%.\".format(perc_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6091804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yup!  That all looks good!\n"
     ]
    }
   ],
   "source": [
    "assert np.round(perc_exp, 2) == 98.55,\\\n",
    "\"Oops!  That doesn't look quite right.You should have total variability as the sum of all the squared elements in the sigma matrix.  Then just the sum of the squared first two elements is the amount explained by the first two latent features.  Try again.\"\n",
    "print(\"Yup!  That all looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b522a1a",
   "metadata": {},
   "source": [
    "`7.` Similar to in the previous question, change the shapes of your u, sigma, and v transpose matrices.  However, this time consider only using the first 2 components to reproduce the user-movie matrix instead of all 4. After you have your matrices set up, check your matrices against the solution by running the tests.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (2 for this case)\n",
    "3. m is the number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3bf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "k = 2\n",
    "u_2 = u[:, :k]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_2 = np.zeros((k, k))\n",
    "s_2[:k, :k] = np.diag(s[:k]) \n",
    "\n",
    "# Because we are using 2 latent features, we need to update vt this time\n",
    "vt_2 = vt[:k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58200ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4. \n",
      "\n",
      " The question is now that we don't have all of the latent features, how well can we really re-create the original user-movie matrix?\n"
     ]
    }
   ],
   "source": [
    "# Check that your matrices are the correct shapes\n",
    "assert u_2.shape == (20, 2),\\\n",
    "\"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 2.\"\n",
    "assert s_2.shape == (2, 2),\\\n",
    "\"Oops!  The shape of the sigma matrix doesn't look right.  It should be 2 x 2.\"\n",
    "assert vt_2.shape == (2, 4),\\\n",
    "\"Oops! The shape of the v transpose matrix doesn't look right.  It should be 2 x 4.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4\")\n",
    "print(\"The question is now that we don't have all of the latent features, how well can we really re-create\")\n",
    "print(\"the original user-movie matrix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1deca2f",
   "metadata": {},
   "source": [
    "`8.` When using all 4 latent features, we saw that we could exactly reproduce the user-movie matrix.  Now that we only have 2 latent features, we might measure how well we are able to reproduce the original matrix by looking at the sum of squared errors from each rating produced by taking the dot product as compared to the actual rating.  Find the sum of squared error based on only the two latent features, and use the following cell to test against the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f070204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product\n",
    "pred_ratings = np.dot(np.dot(u_2, s_2), vt_2)\n",
    "\n",
    "# Compute the squared error for each predicted vs. actual rating\n",
    "sum_square_errs = np.sum(np.sum((user_movie_subset - pred_ratings)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0381f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That looks right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Check against the solution\n",
    "assert np.round(sum_square_errs, 2) == 85.34, \"Oops!  That doesn't look quite right.  You should return a single number for the whole matrix.\"\n",
    "print(\"That looks right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d017b",
   "metadata": {},
   "source": [
    "At this point, you may be thinking... why would we want to choose a k that doesn't just give us back the full user-movie matrix with all the original ratings.  This is a good question.  One reason might be for computational reasons - sure, you may want to reduce the dimensionality of the data you are keeping, but really this isn't the main reason we would want to perform reduce k to lesser than the minimum of the number of movies or users.\n",
    "\n",
    "Let's take a step back for a second.  In this example we just went through, your matrix was very clean.  That is, for every user-movie combination, we had a rating.  **There were no missing values.** But what we know from the previous lesson is that the user-movie matrix is full of missing values.  \n",
    "\n",
    "A matrix similar to the one we just performed SVD on:\n",
    "\n",
    "<img src=\"graphs/nice_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "The real world:\n",
    "\n",
    "<img src=\"graphs/real_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "Therefore, if we keep all k latent features it is likely that latent features with smaller values in the sigma matrix will explain variability that is probably due to noise and not signal. Furthermore, if we use these \"noisey\" latent features to assist in re-constructing the original user-movie matrix it will potentially (and likely) lead to worse ratings than if we only have latent features associated with signal.   \n",
    "\n",
    "`9.` Let's try introducing just a little of the real world into this example by performing SVD on a matrix with missing values.  Below I have added a new user to our matrix who hasn't rated all four of our movies.  Try performing SVD on the new matrix.  What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91cac9",
   "metadata": {},
   "source": [
    "A message will appear as:\n",
    "    \n",
    "`LinAlgError: SVD did not converge` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393f1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line adds one nan value as the very first entry in our matrix\n",
    "#user_movie_subset.iloc[0, 0] = np.nan\n",
    "\n",
    "# Try svd with this new matrix\n",
    "#u, s, vt = np.linalg.svd(user_movie_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c7ecb",
   "metadata": {},
   "source": [
    "**Even with just one nan value we cannot perform SVD!  This is going to be a huge problem, because our real dataset has nan values everywhere!  This is where FunkSVD comes in to help.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e68c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intro to SVD \n",
    "\n",
    "## Second Notebook - L15 - Implementing FunkSVD\n",
    "\n",
    "In this notebook we will take a look at writing our own function that performs FunkSVD, which will follow the steps you saw in the previous video.  If you find that you aren't ready to tackle this task on your own, feel free to skip to the following video where you can watch as I walk through the steps.\n",
    "\n",
    "To test our algorithm, we will run it on the subset of the data you worked with earlier.  Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df35001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18128a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bab8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "# Create data subset\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "ratings_mat = np.matrix(user_movie_subset)\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b4efd",
   "metadata": {},
   "source": [
    "`1.` You will use the **user_movie_subset** matrix to show that your FunkSVD algorithm will converge.  In the below cell, use the comments and document string to assist you as you complete writing your own function to complete FunkSVD.  You may also want to try to complete the funtion on your own without the assistance of comments.  You may feel free to remove and add to the function in any way that gets you a working solution! \n",
    "\n",
    "**Notice:** There isn't a sigma matrix in this version of matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67137a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_FunkSVD(rating_mat, \n",
    "               latent_feature=4, \n",
    "               learning_rate=0.0001, \n",
    "               num_iter=100,\n",
    "               verbose=False):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD \n",
    "    with no regularization\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Second Notebook - Class 15 - Implementing FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - rating_mat (mandatory) - (numpy array) a matrix with users as rows,\n",
    "        movies as columns, and ratings as values\n",
    "      - latent_feature (optional) - (int) the number of latent features used\n",
    "      - learning_rate (optional) - (float) the learning rate \n",
    "      - num_iter (optional) - (int) the number of iterations\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Outputs:\n",
    "      - user_mat - (numpy array) a user by latent feature matrix\n",
    "      - movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function FunkSDV started')\n",
    "    begin = time()\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    num_user = ratings_mat.shape[0]\n",
    "    num_movie = ratings_mat.shape[1]\n",
    "    num_rating = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(num_user, latent_feature)\n",
    "    movie_mat = np.random.rand(latent_feature, num_movie)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # header for running results\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(num_iter):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(num_user):\n",
    "            for j in range(num_movie):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if rating_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = rating_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_feature):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results for iteration\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_rating))\n",
    "        \n",
    "    output = user_mat, movie_mat \n",
    "    \n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f6dda",
   "metadata": {},
   "source": [
    "`2.` Try out your function on the **user_movie_subset** dataset.  First try 4 latent features, a learning rate of 0.005, and 10 iterations.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803480c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function FunkSDV started\n",
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 42.795106\n",
      "2 \t\t 15.478704\n",
      "3 \t\t 4.385920\n",
      "4 \t\t 2.833007\n",
      "5 \t\t 2.656957\n",
      "6 \t\t 2.601192\n",
      "7 \t\t 2.558424\n",
      "8 \t\t 2.513947\n",
      "9 \t\t 2.464330\n",
      "10 \t\t 2.408376\n",
      "elapsed time: 0.08627533912658691s\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = fn_FunkSVD(rating_mat=rating_mat, \n",
    "                                 latent_feature=4, \n",
    "                                 learning_rate=0.005, \n",
    "                                 num_iter=10,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d85e0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.17961853  8.8095889  10.54566021 10.177508  ]\n",
      " [ 8.90042247  7.18618794  9.25762152  8.63711382]\n",
      " [ 7.77616647  6.95487311  8.13595981  7.85830727]\n",
      " [ 9.59212208  8.41580438  9.69983468  9.77537437]\n",
      " [ 8.54036888  7.40405109  9.06645908  8.45201858]\n",
      " [ 7.02646518  5.54619047  7.72967059  6.56017085]\n",
      " [ 9.03920945  8.47068633  9.30415963  9.36912738]\n",
      " [ 8.22198596  6.95697924  9.06506425  7.90439268]\n",
      " [ 8.55906384  7.26462704  9.07007298  8.39064626]\n",
      " [ 7.6332308   6.78892548  7.9751547   7.72549211]\n",
      " [ 8.75419125  7.9295577   9.36237312  8.7973267 ]\n",
      " [ 9.54808354  8.53489967  9.84122333  9.69566286]\n",
      " [ 9.31025033  8.04429566  9.6708381   9.30107622]\n",
      " [ 6.55728073  5.91132277  6.97867197  6.59137964]\n",
      " [ 9.95263372  8.20027365 10.21957317  9.81011095]\n",
      " [ 9.6290559   8.6960672  10.26059189  9.69045093]\n",
      " [ 8.34903737  7.12172486  8.78856366  8.24516913]\n",
      " [ 6.98747133  6.8306299   6.39322166  7.73173   ]\n",
      " [ 8.2674401   7.60519445  8.60638698  8.48116312]\n",
      " [ 8.31715991  6.5982284   9.1123497   7.8125317 ]]\n",
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3064ab",
   "metadata": {},
   "source": [
    "**The predicted ratings from the dot product are already starting to look a lot like the original data values even after only 10 iterations.  You can see some extreme low values that are not captured well yet.  The 5 in the second to last row in the first column is predicted as an 8, and the 4 in the second row and second column is predicted to be a 7.  Clearly the model is not done learning, but things are looking good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330be1f6",
   "metadata": {},
   "source": [
    "`3.` Let's try out the function again on the **user_movie_subset** dataset.  This time we will again use 4 latent features and a learning rate of 0.005.  However, let's bump up the number of iterations to 250.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?  What do you notice about your error at the end of the 250 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47599f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function FunkSDV started\n",
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 41.889987\n",
      "2 \t\t 13.290897\n",
      "3 \t\t 3.778101\n",
      "4 \t\t 2.826693\n",
      "5 \t\t 2.728420\n",
      "6 \t\t 2.694349\n",
      "7 \t\t 2.671833\n",
      "8 \t\t 2.650725\n",
      "9 \t\t 2.628388\n",
      "10 \t\t 2.603938\n",
      "11 \t\t 2.576835\n",
      "12 \t\t 2.546591\n",
      "13 \t\t 2.512713\n",
      "14 \t\t 2.474702\n",
      "15 \t\t 2.432069\n",
      "16 \t\t 2.384347\n",
      "17 \t\t 2.331109\n",
      "18 \t\t 2.272000\n",
      "19 \t\t 2.206764\n",
      "20 \t\t 2.135281\n",
      "21 \t\t 2.057601\n",
      "22 \t\t 1.973987\n",
      "23 \t\t 1.884937\n",
      "24 \t\t 1.791204\n",
      "25 \t\t 1.693791\n",
      "26 \t\t 1.593931\n",
      "27 \t\t 1.493030\n",
      "28 \t\t 1.392589\n",
      "29 \t\t 1.294117\n",
      "30 \t\t 1.199018\n",
      "31 \t\t 1.108507\n",
      "32 \t\t 1.023526\n",
      "33 \t\t 0.944705\n",
      "34 \t\t 0.872347\n",
      "35 \t\t 0.806457\n",
      "36 \t\t 0.746784\n",
      "37 \t\t 0.692889\n",
      "38 \t\t 0.644205\n",
      "39 \t\t 0.600106\n",
      "40 \t\t 0.559951\n",
      "41 \t\t 0.523131\n",
      "42 \t\t 0.489086\n",
      "43 \t\t 0.457330\n",
      "44 \t\t 0.427449\n",
      "45 \t\t 0.399110\n",
      "46 \t\t 0.372054\n",
      "47 \t\t 0.346088\n",
      "48 \t\t 0.321082\n",
      "49 \t\t 0.296961\n",
      "50 \t\t 0.273692\n",
      "51 \t\t 0.251279\n",
      "52 \t\t 0.229751\n",
      "53 \t\t 0.209158\n",
      "54 \t\t 0.189558\n",
      "55 \t\t 0.171010\n",
      "56 \t\t 0.153571\n",
      "57 \t\t 0.137286\n",
      "58 \t\t 0.122187\n",
      "59 \t\t 0.108288\n",
      "60 \t\t 0.095587\n",
      "61 \t\t 0.084062\n",
      "62 \t\t 0.073676\n",
      "63 \t\t 0.064377\n",
      "64 \t\t 0.056104\n",
      "65 \t\t 0.048784\n",
      "66 \t\t 0.042341\n",
      "67 \t\t 0.036696\n",
      "68 \t\t 0.031771\n",
      "69 \t\t 0.027489\n",
      "70 \t\t 0.023776\n",
      "71 \t\t 0.020565\n",
      "72 \t\t 0.017793\n",
      "73 \t\t 0.015403\n",
      "74 \t\t 0.013345\n",
      "75 \t\t 0.011572\n",
      "76 \t\t 0.010045\n",
      "77 \t\t 0.008730\n",
      "78 \t\t 0.007597\n",
      "79 \t\t 0.006619\n",
      "80 \t\t 0.005775\n",
      "81 \t\t 0.005045\n",
      "82 \t\t 0.004413\n",
      "83 \t\t 0.003865\n",
      "84 \t\t 0.003389\n",
      "85 \t\t 0.002975\n",
      "86 \t\t 0.002615\n",
      "87 \t\t 0.002300\n",
      "88 \t\t 0.002026\n",
      "89 \t\t 0.001785\n",
      "90 \t\t 0.001575\n",
      "91 \t\t 0.001390\n",
      "92 \t\t 0.001228\n",
      "93 \t\t 0.001085\n",
      "94 \t\t 0.000960\n",
      "95 \t\t 0.000850\n",
      "96 \t\t 0.000752\n",
      "97 \t\t 0.000666\n",
      "98 \t\t 0.000590\n",
      "99 \t\t 0.000523\n",
      "100 \t\t 0.000464\n",
      "101 \t\t 0.000411\n",
      "102 \t\t 0.000365\n",
      "103 \t\t 0.000324\n",
      "104 \t\t 0.000287\n",
      "105 \t\t 0.000255\n",
      "106 \t\t 0.000227\n",
      "107 \t\t 0.000201\n",
      "108 \t\t 0.000179\n",
      "109 \t\t 0.000159\n",
      "110 \t\t 0.000141\n",
      "111 \t\t 0.000125\n",
      "112 \t\t 0.000111\n",
      "113 \t\t 0.000099\n",
      "114 \t\t 0.000088\n",
      "115 \t\t 0.000078\n",
      "116 \t\t 0.000069\n",
      "117 \t\t 0.000062\n",
      "118 \t\t 0.000055\n",
      "119 \t\t 0.000049\n",
      "120 \t\t 0.000043\n",
      "121 \t\t 0.000038\n",
      "122 \t\t 0.000034\n",
      "123 \t\t 0.000030\n",
      "124 \t\t 0.000027\n",
      "125 \t\t 0.000024\n",
      "126 \t\t 0.000021\n",
      "127 \t\t 0.000019\n",
      "128 \t\t 0.000017\n",
      "129 \t\t 0.000015\n",
      "130 \t\t 0.000013\n",
      "131 \t\t 0.000012\n",
      "132 \t\t 0.000011\n",
      "133 \t\t 0.000009\n",
      "134 \t\t 0.000008\n",
      "135 \t\t 0.000007\n",
      "136 \t\t 0.000007\n",
      "137 \t\t 0.000006\n",
      "138 \t\t 0.000005\n",
      "139 \t\t 0.000005\n",
      "140 \t\t 0.000004\n",
      "141 \t\t 0.000004\n",
      "142 \t\t 0.000003\n",
      "143 \t\t 0.000003\n",
      "144 \t\t 0.000003\n",
      "145 \t\t 0.000002\n",
      "146 \t\t 0.000002\n",
      "147 \t\t 0.000002\n",
      "148 \t\t 0.000002\n",
      "149 \t\t 0.000001\n",
      "150 \t\t 0.000001\n",
      "151 \t\t 0.000001\n",
      "152 \t\t 0.000001\n",
      "153 \t\t 0.000001\n",
      "154 \t\t 0.000001\n",
      "155 \t\t 0.000001\n",
      "156 \t\t 0.000001\n",
      "157 \t\t 0.000001\n",
      "158 \t\t 0.000000\n",
      "159 \t\t 0.000000\n",
      "160 \t\t 0.000000\n",
      "161 \t\t 0.000000\n",
      "162 \t\t 0.000000\n",
      "163 \t\t 0.000000\n",
      "164 \t\t 0.000000\n",
      "165 \t\t 0.000000\n",
      "166 \t\t 0.000000\n",
      "167 \t\t 0.000000\n",
      "168 \t\t 0.000000\n",
      "169 \t\t 0.000000\n",
      "170 \t\t 0.000000\n",
      "171 \t\t 0.000000\n",
      "172 \t\t 0.000000\n",
      "173 \t\t 0.000000\n",
      "174 \t\t 0.000000\n",
      "175 \t\t 0.000000\n",
      "176 \t\t 0.000000\n",
      "177 \t\t 0.000000\n",
      "178 \t\t 0.000000\n",
      "179 \t\t 0.000000\n",
      "180 \t\t 0.000000\n",
      "181 \t\t 0.000000\n",
      "182 \t\t 0.000000\n",
      "183 \t\t 0.000000\n",
      "184 \t\t 0.000000\n",
      "185 \t\t 0.000000\n",
      "186 \t\t 0.000000\n",
      "187 \t\t 0.000000\n",
      "188 \t\t 0.000000\n",
      "189 \t\t 0.000000\n",
      "190 \t\t 0.000000\n",
      "191 \t\t 0.000000\n",
      "192 \t\t 0.000000\n",
      "193 \t\t 0.000000\n",
      "194 \t\t 0.000000\n",
      "195 \t\t 0.000000\n",
      "196 \t\t 0.000000\n",
      "197 \t\t 0.000000\n",
      "198 \t\t 0.000000\n",
      "199 \t\t 0.000000\n",
      "200 \t\t 0.000000\n",
      "201 \t\t 0.000000\n",
      "202 \t\t 0.000000\n",
      "203 \t\t 0.000000\n",
      "204 \t\t 0.000000\n",
      "205 \t\t 0.000000\n",
      "206 \t\t 0.000000\n",
      "207 \t\t 0.000000\n",
      "208 \t\t 0.000000\n",
      "209 \t\t 0.000000\n",
      "210 \t\t 0.000000\n",
      "211 \t\t 0.000000\n",
      "212 \t\t 0.000000\n",
      "213 \t\t 0.000000\n",
      "214 \t\t 0.000000\n",
      "215 \t\t 0.000000\n",
      "216 \t\t 0.000000\n",
      "217 \t\t 0.000000\n",
      "218 \t\t 0.000000\n",
      "219 \t\t 0.000000\n",
      "220 \t\t 0.000000\n",
      "221 \t\t 0.000000\n",
      "222 \t\t 0.000000\n",
      "223 \t\t 0.000000\n",
      "224 \t\t 0.000000\n",
      "225 \t\t 0.000000\n",
      "226 \t\t 0.000000\n",
      "227 \t\t 0.000000\n",
      "228 \t\t 0.000000\n",
      "229 \t\t 0.000000\n",
      "230 \t\t 0.000000\n",
      "231 \t\t 0.000000\n",
      "232 \t\t 0.000000\n",
      "233 \t\t 0.000000\n",
      "234 \t\t 0.000000\n",
      "235 \t\t 0.000000\n",
      "236 \t\t 0.000000\n",
      "237 \t\t 0.000000\n",
      "238 \t\t 0.000000\n",
      "239 \t\t 0.000000\n",
      "240 \t\t 0.000000\n",
      "241 \t\t 0.000000\n",
      "242 \t\t 0.000000\n",
      "243 \t\t 0.000000\n",
      "244 \t\t 0.000000\n",
      "245 \t\t 0.000000\n",
      "246 \t\t 0.000000\n",
      "247 \t\t 0.000000\n",
      "248 \t\t 0.000000\n",
      "249 \t\t 0.000000\n",
      "250 \t\t 0.000000\n",
      "elapsed time: 1.0133740901947021s\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = fn_FunkSVD(rating_mat=rating_mat, \n",
    "                                 latent_feature=4, \n",
    "                                 learning_rate=0.005, \n",
    "                                 num_iter=250,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12bd5801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.00000049 10.00000014 10.00000006  9.99999958]\n",
      " [ 9.99999753  3.99999822  8.99999934 10.00000293]\n",
      " [ 7.99999397  8.99999604  9.99999854  5.00000687]\n",
      " [ 8.9999964   7.99999773  9.99999924 10.00000413]\n",
      " [ 9.9999967   4.99999799  8.99999935  9.00000378]\n",
      " [ 5.99999993  4.0000001  10.00000013  6.00000014]\n",
      " [ 8.99999694  7.99999835  9.99999951  9.00000342]\n",
      " [10.00000176  5.00000137  9.00000064  7.99999806]\n",
      " [ 6.99999616  7.99999772  9.9999993   8.00000438]\n",
      " [ 9.00000662  5.00000448  9.00000177  6.99999259]\n",
      " [ 9.00000153  8.00000106 10.0000005   7.99999838]\n",
      " [ 9.00000546 10.00000363 10.00000145  8.99999392]\n",
      " [ 9.99999755  8.99999842  9.99999942  8.0000028 ]\n",
      " [ 5.00000371  8.0000026   5.00000099  7.99999573]\n",
      " [ 9.99999958  7.99999962  9.99999987 10.00000058]\n",
      " [ 9.00000071  9.00000046 10.00000017  9.99999924]\n",
      " [ 9.0000029   8.00000179  8.00000066  7.99999681]\n",
      " [ 9.99999814  7.99999879  0.99999951 10.00000203]\n",
      " [ 4.99999905  5.9999993   9.99999972 10.00000113]\n",
      " [ 8.00000077  7.00000042 10.00000015  6.99999922]]\n",
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5cfaf4",
   "metadata": {},
   "source": [
    "**In this case, we were able to completely reconstruct the item-movie matrix to obtain an essentially 0 mean squared error. I obtained 0 MSE on iteration 165.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2551d1",
   "metadata": {},
   "source": [
    "The last time we placed an **np.nan** value into this matrix the entire svd algorithm in python broke.  Let's see if that is still the case using your FunkSVD function.  In the below cell, I have placed a nan into the first cell of your numpy array.  \n",
    "\n",
    "`4.` Use 4 latent features, a learning rate of 0.005, and 250 iterations.  Are you able to run your SVD without it breaking (something that was not true about the python built in)?  Do you get a prediction for the nan value?  What is your prediction for the missing value? Use the cells below to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97d8855a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[nan, 10., 10., 10.],\n",
       "        [10.,  4.,  9., 10.],\n",
       "        [ 8.,  9., 10.,  5.],\n",
       "        [ 9.,  8., 10., 10.],\n",
       "        [10.,  5.,  9.,  9.],\n",
       "        [ 6.,  4., 10.,  6.],\n",
       "        [ 9.,  8., 10.,  9.],\n",
       "        [10.,  5.,  9.,  8.],\n",
       "        [ 7.,  8., 10.,  8.],\n",
       "        [ 9.,  5.,  9.,  7.],\n",
       "        [ 9.,  8., 10.,  8.],\n",
       "        [ 9., 10., 10.,  9.],\n",
       "        [10.,  9., 10.,  8.],\n",
       "        [ 5.,  8.,  5.,  8.],\n",
       "        [10.,  8., 10., 10.],\n",
       "        [ 9.,  9., 10., 10.],\n",
       "        [ 9.,  8.,  8.,  8.],\n",
       "        [10.,  8.,  1., 10.],\n",
       "        [ 5.,  6., 10., 10.],\n",
       "        [ 8.,  7., 10.,  7.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1733a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function FunkSDV started\n",
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 47.263018\n",
      "2 \t\t 18.235082\n",
      "3 \t\t 4.921511\n",
      "4 \t\t 3.000977\n",
      "5 \t\t 2.810490\n",
      "6 \t\t 2.763808\n",
      "7 \t\t 2.737815\n",
      "8 \t\t 2.715185\n",
      "9 \t\t 2.691752\n",
      "10 \t\t 2.666098\n",
      "11 \t\t 2.637429\n",
      "12 \t\t 2.605097\n",
      "13 \t\t 2.568498\n",
      "14 \t\t 2.527052\n",
      "15 \t\t 2.480229\n",
      "16 \t\t 2.427567\n",
      "17 \t\t 2.368721\n",
      "18 \t\t 2.303507\n",
      "19 \t\t 2.231946\n",
      "20 \t\t 2.154315\n",
      "21 \t\t 2.071173\n",
      "22 \t\t 1.983374\n",
      "23 \t\t 1.892045\n",
      "24 \t\t 1.798530\n",
      "25 \t\t 1.704307\n",
      "26 \t\t 1.610874\n",
      "27 \t\t 1.519645\n",
      "28 \t\t 1.431842\n",
      "29 \t\t 1.348436\n",
      "30 \t\t 1.270109\n",
      "31 \t\t 1.197265\n",
      "32 \t\t 1.130067\n",
      "33 \t\t 1.068490\n",
      "34 \t\t 1.012373\n",
      "35 \t\t 0.961468\n",
      "36 \t\t 0.915476\n",
      "37 \t\t 0.874071\n",
      "38 \t\t 0.836913\n",
      "39 \t\t 0.803655\n",
      "40 \t\t 0.773946\n",
      "41 \t\t 0.747437\n",
      "42 \t\t 0.723782\n",
      "43 \t\t 0.702644\n",
      "44 \t\t 0.683694\n",
      "45 \t\t 0.666621\n",
      "46 \t\t 0.651130\n",
      "47 \t\t 0.636948\n",
      "48 \t\t 0.623821\n",
      "49 \t\t 0.611518\n",
      "50 \t\t 0.599828\n",
      "51 \t\t 0.588560\n",
      "52 \t\t 0.577541\n",
      "53 \t\t 0.566614\n",
      "54 \t\t 0.555636\n",
      "55 \t\t 0.544479\n",
      "56 \t\t 0.533025\n",
      "57 \t\t 0.521169\n",
      "58 \t\t 0.508818\n",
      "59 \t\t 0.495886\n",
      "60 \t\t 0.482305\n",
      "61 \t\t 0.468016\n",
      "62 \t\t 0.452977\n",
      "63 \t\t 0.437160\n",
      "64 \t\t 0.420558\n",
      "65 \t\t 0.403185\n",
      "66 \t\t 0.385073\n",
      "67 \t\t 0.366282\n",
      "68 \t\t 0.346894\n",
      "69 \t\t 0.327015\n",
      "70 \t\t 0.306773\n",
      "71 \t\t 0.286317\n",
      "72 \t\t 0.265812\n",
      "73 \t\t 0.245431\n",
      "74 \t\t 0.225354\n",
      "75 \t\t 0.205760\n",
      "76 \t\t 0.186816\n",
      "77 \t\t 0.168676\n",
      "78 \t\t 0.151472\n",
      "79 \t\t 0.135311\n",
      "80 \t\t 0.120271\n",
      "81 \t\t 0.106401\n",
      "82 \t\t 0.093723\n",
      "83 \t\t 0.082229\n",
      "84 \t\t 0.071891\n",
      "85 \t\t 0.062659\n",
      "86 \t\t 0.054469\n",
      "87 \t\t 0.047248\n",
      "88 \t\t 0.040915\n",
      "89 \t\t 0.035385\n",
      "90 \t\t 0.030577\n",
      "91 \t\t 0.026410\n",
      "92 \t\t 0.022807\n",
      "93 \t\t 0.019700\n",
      "94 \t\t 0.017023\n",
      "95 \t\t 0.014719\n",
      "96 \t\t 0.012738\n",
      "97 \t\t 0.011034\n",
      "98 \t\t 0.009568\n",
      "99 \t\t 0.008306\n",
      "100 \t\t 0.007219\n",
      "101 \t\t 0.006282\n",
      "102 \t\t 0.005472\n",
      "103 \t\t 0.004773\n",
      "104 \t\t 0.004167\n",
      "105 \t\t 0.003643\n",
      "106 \t\t 0.003187\n",
      "107 \t\t 0.002791\n",
      "108 \t\t 0.002446\n",
      "109 \t\t 0.002146\n",
      "110 \t\t 0.001884\n",
      "111 \t\t 0.001655\n",
      "112 \t\t 0.001455\n",
      "113 \t\t 0.001280\n",
      "114 \t\t 0.001126\n",
      "115 \t\t 0.000992\n",
      "116 \t\t 0.000874\n",
      "117 \t\t 0.000770\n",
      "118 \t\t 0.000679\n",
      "119 \t\t 0.000599\n",
      "120 \t\t 0.000528\n",
      "121 \t\t 0.000466\n",
      "122 \t\t 0.000411\n",
      "123 \t\t 0.000363\n",
      "124 \t\t 0.000321\n",
      "125 \t\t 0.000283\n",
      "126 \t\t 0.000250\n",
      "127 \t\t 0.000221\n",
      "128 \t\t 0.000195\n",
      "129 \t\t 0.000172\n",
      "130 \t\t 0.000152\n",
      "131 \t\t 0.000135\n",
      "132 \t\t 0.000119\n",
      "133 \t\t 0.000105\n",
      "134 \t\t 0.000093\n",
      "135 \t\t 0.000082\n",
      "136 \t\t 0.000073\n",
      "137 \t\t 0.000064\n",
      "138 \t\t 0.000057\n",
      "139 \t\t 0.000050\n",
      "140 \t\t 0.000044\n",
      "141 \t\t 0.000039\n",
      "142 \t\t 0.000035\n",
      "143 \t\t 0.000031\n",
      "144 \t\t 0.000027\n",
      "145 \t\t 0.000024\n",
      "146 \t\t 0.000021\n",
      "147 \t\t 0.000019\n",
      "148 \t\t 0.000017\n",
      "149 \t\t 0.000015\n",
      "150 \t\t 0.000013\n",
      "151 \t\t 0.000012\n",
      "152 \t\t 0.000010\n",
      "153 \t\t 0.000009\n",
      "154 \t\t 0.000008\n",
      "155 \t\t 0.000007\n",
      "156 \t\t 0.000006\n",
      "157 \t\t 0.000006\n",
      "158 \t\t 0.000005\n",
      "159 \t\t 0.000004\n",
      "160 \t\t 0.000004\n",
      "161 \t\t 0.000003\n",
      "162 \t\t 0.000003\n",
      "163 \t\t 0.000003\n",
      "164 \t\t 0.000002\n",
      "165 \t\t 0.000002\n",
      "166 \t\t 0.000002\n",
      "167 \t\t 0.000002\n",
      "168 \t\t 0.000001\n",
      "169 \t\t 0.000001\n",
      "170 \t\t 0.000001\n",
      "171 \t\t 0.000001\n",
      "172 \t\t 0.000001\n",
      "173 \t\t 0.000001\n",
      "174 \t\t 0.000001\n",
      "175 \t\t 0.000001\n",
      "176 \t\t 0.000001\n",
      "177 \t\t 0.000000\n",
      "178 \t\t 0.000000\n",
      "179 \t\t 0.000000\n",
      "180 \t\t 0.000000\n",
      "181 \t\t 0.000000\n",
      "182 \t\t 0.000000\n",
      "183 \t\t 0.000000\n",
      "184 \t\t 0.000000\n",
      "185 \t\t 0.000000\n",
      "186 \t\t 0.000000\n",
      "187 \t\t 0.000000\n",
      "188 \t\t 0.000000\n",
      "189 \t\t 0.000000\n",
      "190 \t\t 0.000000\n",
      "191 \t\t 0.000000\n",
      "192 \t\t 0.000000\n",
      "193 \t\t 0.000000\n",
      "194 \t\t 0.000000\n",
      "195 \t\t 0.000000\n",
      "196 \t\t 0.000000\n",
      "197 \t\t 0.000000\n",
      "198 \t\t 0.000000\n",
      "199 \t\t 0.000000\n",
      "200 \t\t 0.000000\n",
      "201 \t\t 0.000000\n",
      "202 \t\t 0.000000\n",
      "203 \t\t 0.000000\n",
      "204 \t\t 0.000000\n",
      "205 \t\t 0.000000\n",
      "206 \t\t 0.000000\n",
      "207 \t\t 0.000000\n",
      "208 \t\t 0.000000\n",
      "209 \t\t 0.000000\n",
      "210 \t\t 0.000000\n",
      "211 \t\t 0.000000\n",
      "212 \t\t 0.000000\n",
      "213 \t\t 0.000000\n",
      "214 \t\t 0.000000\n",
      "215 \t\t 0.000000\n",
      "216 \t\t 0.000000\n",
      "217 \t\t 0.000000\n",
      "218 \t\t 0.000000\n",
      "219 \t\t 0.000000\n",
      "220 \t\t 0.000000\n",
      "221 \t\t 0.000000\n",
      "222 \t\t 0.000000\n",
      "223 \t\t 0.000000\n",
      "224 \t\t 0.000000\n",
      "225 \t\t 0.000000\n",
      "226 \t\t 0.000000\n",
      "227 \t\t 0.000000\n",
      "228 \t\t 0.000000\n",
      "229 \t\t 0.000000\n",
      "230 \t\t 0.000000\n",
      "231 \t\t 0.000000\n",
      "232 \t\t 0.000000\n",
      "233 \t\t 0.000000\n",
      "234 \t\t 0.000000\n",
      "235 \t\t 0.000000\n",
      "236 \t\t 0.000000\n",
      "237 \t\t 0.000000\n",
      "238 \t\t 0.000000\n",
      "239 \t\t 0.000000\n",
      "240 \t\t 0.000000\n",
      "241 \t\t 0.000000\n",
      "242 \t\t 0.000000\n",
      "243 \t\t 0.000000\n",
      "244 \t\t 0.000000\n",
      "245 \t\t 0.000000\n",
      "246 \t\t 0.000000\n",
      "247 \t\t 0.000000\n",
      "248 \t\t 0.000000\n",
      "249 \t\t 0.000000\n",
      "250 \t\t 0.000000\n",
      "elapsed time: 1.1802723407745361s\n"
     ]
    }
   ],
   "source": [
    "# run SVD on the matrix with the missing value\n",
    "user_mat, movie_mat = fn_FunkSVD(ratings_mat=ratings_mat, \n",
    "                                 latent_features=4, \n",
    "                                 learning_rate=0.005, \n",
    "                                 num_iter=250,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55dbe5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the missing rating is 8.817766277626827:\n",
      "\n",
      "The actual value for the missing rating is nan:\n",
      "\n",
      "That's right! You just predicted a rating for a user-movie pair that was never rated!\n",
      "But if you look in the original matrix, this was actually a value of 10. Not bad!\n"
     ]
    }
   ],
   "source": [
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is {}:\".format(preds[0,0]))\n",
    "print()\n",
    "print(\"The actual value for the missing rating is {}:\".format(ratings_mat[0,0]))\n",
    "print()\n",
    "assert np.isnan(preds[0,0]) == False\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 10. Not bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61220b4e",
   "metadata": {},
   "source": [
    "Now let's extend this to a more realistic example. Unfortunately, running this function on your entire user-movie matrix is still not something you likely want to do on your local machine.  However, we can see how well this example extends to 1000 users.  In the above portion, you were using a very small subset of data with no missing values.\n",
    "\n",
    "`5.` Given the size of this matrix, this will take quite a bit of time.  Consider the following hyperparameters: 4 latent features, 0.005 learning rate, and 20 iterations.  Grab a snack, take a walk, and this should be done running in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "983e2236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###function FunkSDV started\n",
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 23.225430\n",
      "2 \t\t 10.663682\n",
      "3 \t\t 7.315128\n",
      "4 \t\t 5.650354\n",
      "5 \t\t 4.601433\n",
      "6 \t\t 3.868078\n",
      "7 \t\t 3.323309\n",
      "8 \t\t 2.902176\n",
      "9 \t\t 2.567283\n",
      "10 \t\t 2.295316\n",
      "11 \t\t 2.070935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3280/3401665088.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# perform funkSVD on the matrix of the top 1000 users\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m user_mat, movie_mat = fn_FunkSVD(ratings_mat=first_1000_users, \n\u001b[0m\u001b[0;32m      6\u001b[0m                                  \u001b[0mlatent_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3280/856199132.py\u001b[0m in \u001b[0;36mfn_FunkSVD\u001b[1;34m(ratings_mat, latent_features, learning_rate, iters, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# if the rating exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mratings_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[1;31m# compute the error as the actual minus the dot product of the user and movie latent features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setting up a matrix of the first 1000 users with movie ratings\n",
    "first_1000_user = np.matrix(user_by_movie.head(1000))\n",
    "\n",
    "# perform funkSVD on the matrix of the top 1000 users\n",
    "user_mat, movie_mat = fn_FunkSVD(rating_mat=first_1000_user, \n",
    "                                 latent_feature=4, \n",
    "                                 learning_rate=0.005, \n",
    "                                 num_iter=20,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3af80",
   "metadata": {},
   "source": [
    "`6.` Now that you have a set of predictions for each user-movie pair.  Let's answer a few questions about your results. Provide the correct values to each of the variables below, and check your solutions using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many actual ratings exist in first_1000_users\n",
    "num_rating = np.count_nonzero(~np.isnan(first_1000_user))\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_rating))\n",
    "print()\n",
    "\n",
    "# How many ratings did we make for user-movie pairs that didn't have ratings\n",
    "rating_for_missing = first_1000_user.shape[0]*first_1000_users.shape[1] - num_rating\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(rating_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your results against the solution\n",
    "assert num_rating == 10852, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert rating_for_missing == 31234148, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "pred = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(pred).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdea61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Metrics on prediction \n",
    "\n",
    "## Third Notebook - L18 - How Are We Doing?\n",
    "\n",
    "In the last notebook, you created a working version of SVD for situations even when there are tons of missing values.  This is awesome!  The question now is how well does this solution work?\n",
    "\n",
    "In this notebook, we are going to simulate exactly what we would do in the real world to tune our recommender.  \n",
    "\n",
    "Run the cell below to read in the data and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d2b73",
   "metadata": {},
   "source": [
    "1. Using the **reviews** dataframe, perform the following tasks to create a training and validation set of data we can use to test the performance of your SVD algorithm using **off-line** validation techniques.\n",
    "\n",
    " * Order the reviews dataframe from earliest to most recent \n",
    " * Pull the first 10000 reviews from  the dataset\n",
    " * Make the first 8000/10000 reviews the training data \n",
    " * Make the last 2000/10000 the test data\n",
    " * Return the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_create_train_test(reviews, \n",
    "                         order_by, \n",
    "                         training_size, \n",
    "                         testing_size,\n",
    "                         verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Third Notebook - Class 18 - How are we doing w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - reviews (mandatory) - (pandas df) dataframe to split into train and test\n",
    "      - order_by (mandatory) - (string) column name to sort by\n",
    "      - training_size (mandatory) - (int) number of rows in training set\n",
    "      - testing_size (mandatory) - (int) number of columns in the test set\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Outputs:\n",
    "      - training_df -  (pandas df) dataframe of the training set\n",
    "      - validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function create_train_test started')\n",
    "    begin = time()\n",
    "\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    output = training_df, validation_df\n",
    "    \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d64606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = fn_create_train_test(reviews=reviews, \n",
    "                                        order_by='date', \n",
    "                                        training_size=8000, \n",
    "                                        testing_size=2000,\n",
    "                                        verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the dataframes we are using are the right shape\n",
    "assert train_df.shape[0] == 8000,\\\n",
    "\"The number of rows doesn't look right in the training dataset.\"\n",
    "assert val_df.shape[0] == 2000,\\\n",
    "\"The number of rows doesn't look right in the validation dataset\"\n",
    "assert str(train_df.tail(1)['date']).split()[1] == '2013-03-15',\n",
    "    \"The last date in the training dataset doesn't look like what we expected.\"\n",
    "assert str(val_df.tail(1)['date']).split()[1] == '2013-03-18',\n",
    "    \"The last date in the validation dataset doesn't look like what we expected.\"\n",
    "print(\"Nice job!  Looks like you have written a function that provides training and validation dataframes\")\n",
    "print(\"for you to use in the next steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf92f5",
   "metadata": {},
   "source": [
    "In the real world, we might have all of the data up to this final date in the training data.  Then we want to see how well we are doing for each of the new ratings, which show up in the test data.\n",
    "\n",
    "Below is a working example of the function created in the previous example you can use (or you can replace with your own).\n",
    "\n",
    "`2.`  Fit the function to the training data with the following hyperparameters: 15 latent features, a learning rate of 0.005, and 250 iterations. This will take some time to run, so you may choose fewer latent features, a higher learning rate, or fewer iteratios if you want to speed up the process.  \n",
    "\n",
    "**Note:** Again, this might be a good time to take a phone call, go for a walk, or just take a little break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_FunkSVD(ratings_mat, \n",
    "               latent_features=12, \n",
    "               learning_rate=0.0001, \n",
    "               iters=100,\n",
    "               verbose=False):\n",
    "    '''This function performs matrix factorization using a basic form of FunkSVD \n",
    "    with no regularization.\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Third Notebook - Class 18 - How are we doing w/ FunkSVD\n",
    "\n",
    "    Inputs:\n",
    "      - ratings_mat (mandatory) - (numpy array) a matrix with users as rows, \n",
    "        movies as columns, and ratings as values\n",
    "      - latent_features (optional) - (int) the number of latent features used\n",
    "      - learning_rate (optional) - (float) the learning rate \n",
    "      - iters (optional) - (int) the number of iterations\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)    \n",
    "    Outputs:\n",
    "      - user_mat - (numpy array) a user by latent feature matrix\n",
    "      - movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    output = user_mat, movie_mat\n",
    "        \n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-by-item matrix - nothing to do here\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Fit FunkSVD with the specified hyper parameters to the training data\n",
    "user_mat, movie_mat = fn_FunkSVD(ratings_mat=train_data_np, \n",
    "                                 latent_features=15, \n",
    "                                 learning_rate=0.005, \n",
    "                                 iters=250,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6286ac",
   "metadata": {},
   "source": [
    "Now that you have created the **user_mat** and **movie_mat**, we can use this to make predictions for how users would rate movies, by just computing the dot product of the row associated with a user and the column associated with the movie.\n",
    "\n",
    "`3.` Use the comments in the function below to complete the **predict_rating** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33940e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_predict_rating(user_matrix, \n",
    "                      movie_matrix, \n",
    "                      user_id, \n",
    "                      movie_id,\n",
    "                      verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Third Notebook - Class 18 - How are we doing w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - user_matrix (mandatory) - user by latent factor matrix\n",
    "      - movie_matrix (mandatory) - latent factor by movie matrix\n",
    "      - user_id (mandatory) - the user_id from the reviews df\n",
    "      - movie_id (mandatory) - the movie_id according the movies df\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)    \n",
    "    Output:\n",
    "    pred \n",
    "      - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    # Use the training data to create a series of users and movies that matches the ordering in training data\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    \n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c290b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function with the first user-movie in the user-movie matrix (notice this is a nan)\n",
    "pred_val = fn_predict_rating(user_matrix=user_mat, \n",
    "                             movie_matrix=movie_mat, \n",
    "                             user_id=8,\n",
    "                             movie_id=2844,\n",
    "                             verbose=True)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91a589",
   "metadata": {},
   "source": [
    "It is great that you now have a way to make predictions. However it might be nice to get a little phrase back about the user, movie, and rating.\n",
    "\n",
    "`4.` Use the comments in the function below to complete the **predict_rating** function.  \n",
    "\n",
    "**Note:** The movie name doesn't come back in a great format, so you can see in the solution I messed around with it a bit just to make it a little nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5adc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_print_prediction_summary(user_id, \n",
    "                                movie_id, \n",
    "                                prediction,\n",
    "                                verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Third Notebook - Class 18 - How are we doing w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - user_id (mandatory) - the user_id from the reviews df\n",
    "      - movie_id (mandatory) - the movie_id according the movies df\n",
    "      - prediction (mandatory) - the predicted rating for user_id-movie_id\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)    \n",
    "    Output:\n",
    "      - True, if the function runs well - this is a relatory function!\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    movie_name = str(movies[movies['movie_id'] == movie_id]['movie']) [5:]\n",
    "    movie_name = movie_name.replace('\\nName: movie, dtype: object', '')\n",
    "    print(\"For user {} we predict a {} rating for the movie {}.\".format(user_id, round(prediction, 2), str(movie_name)))\n",
    "\n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function the the results of the previous function\n",
    "fn_print_prediction_summary(user_id=8, \n",
    "                            movie_id=2844, \n",
    "                            prediction=pred_val,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4447",
   "metadata": {},
   "source": [
    "Now that we have the ability to make predictions, let's see how well our predictions do on the test ratings we already have.  This will give an indication of how well have captured the latent features, and our ability to use the latent features to make predictions in the future!\n",
    "\n",
    "`5.` For each of the user-movie rating in the **val_df** dataset, compare the actual rating given to the prediction you would make.  How do your predictions do?  Do you run into any problems?  If yes, what is the problem?  Use the document strings and comments below to assist as you work through these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_validation_comparison(val_df, \n",
    "                             num_preds,\n",
    "                             verbose=False):\n",
    "    '''This function prints a relatory for the prediction made, row by row of\n",
    "    val_df.\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Third Notebook - Class 18 - How are we doing w/ FunkSVD\n",
    "    \n",
    "    Input:\n",
    "      - val_df (mandatory) - the validation dataset created in the third cell \n",
    "        above\n",
    "      - num_preds (mandatory) - (int) the number of rows (going in order) \n",
    "        you would like to make predictions for\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - return True if erverything goes well - this is a relatory function!\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function validation comparison started')\n",
    "    begin = time()\n",
    "\n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    for idx in range(num_preds):\n",
    "        pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "        print(\"The actual rating for user {} on movie {} is {}.\\n While the predicted rating is {}.\".format(val_users[idx], val_movies[idx], val_ratings[idx], round(pred))) \n",
    "\n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d14fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the predicted vs. actual for the first 6 rows.  How does it look?\n",
    "fn_validation_comparison(val_df=val_df, \n",
    "                         num_pred=6,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764db75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the predicted vs. actual for the first 7 rows.  What happened?\n",
    "fn_validation_comparison(val_df=val_df, \n",
    "                         num_pred=7,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fe2c8",
   "metadata": {},
   "source": [
    "**The 7th movie is a movie that has no ratings.  Therefore, we are not able to make a prediction for this user-movie pair.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e224389",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Users with no Data\n",
    "\n",
    "## Forth Notebook - L20 - Cold Start Problem\n",
    "\n",
    "In the previous notebook, you learned about the **Cold Start Problem** first hand. In cases where you are introduced to a new user or new movie, collaborative flitering is not helpful as a technique to make predictions.\n",
    "\n",
    "Instead, you will need to use one of the techniques from the previous lesson like content based recommendations for new items or rank based recommendations for new users.  \n",
    "\n",
    "As a final step to completing out our recommendation system, we will build in these edge cases. Run the cell below to get started.\n",
    "\n",
    "### Matrix Factorization - Collaborative Filtering Where Possible\n",
    "\n",
    "Notice the following information is available by running the below cell:\n",
    "\n",
    "`1.` **reviews** - a dataframe of reviews\n",
    "\n",
    "`2.` **movies** - a dataframe of movies\n",
    "\n",
    "`3.` **create_train_test** - a function for creating the training and validation datasets\n",
    "\n",
    "`4.` **predict_rating** - a function that takes a user and movie and gives a prediction using FunkSVD\n",
    "\n",
    "`5.` **train_df** and **val_df** - the training and test datasets used in the previous notebook\n",
    "\n",
    "`6.` **user_mat** and **movie_mat** - the u and v matrices from FunkSVD\n",
    "\n",
    "`7.` **train_data_df** - a user-movie matrix with ratings where available.  FunkSVD was performed on this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "movie = udacourse3.fn_read_data('data/movies_clean.csv', remove_noisy_cols=True)\n",
    "review = udacourse3.fn_read_data('data/reviews_clean.csv', remove_noisy_cols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_create_train_test(review, \n",
    "                         order_by, \n",
    "                         training_size, \n",
    "                         testing_size,\n",
    "                         verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - reviews (mandatory) - (pandas df) dataframe to split into train and test\n",
    "      - order_by (mandatory) - (string) column name to sort by\n",
    "      - training_size (mandatory) - (int) number of rows in training set\n",
    "      - testing_size (mandatory) - (int) number of columns in the test set\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Outputs:\n",
    "      - training_df - (pandas df) dataframe of the training set\n",
    "      - validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    output = training_df, validation_df\n",
    "    \n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f431df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_predict_rating(user_matrix, \n",
    "                      movie_matrix, \n",
    "                      user_id, \n",
    "                      movie_id,\n",
    "                      verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - user_matrix (mandatory) - user by latent factor matrix\n",
    "      - movie_matrix (mandatory) - latent factor by movie matrix\n",
    "      - user_id (mandatory) - the user_id from the reviews df\n",
    "      - movie_id (mandatory) - the movie_id according the movies df\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    # Create series of users and movies in the right order\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "\n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-start))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = fn_create_train_test(reviews=reviews, \n",
    "                                        order_by='date', \n",
    "                                        training_size=8000, \n",
    "                                        testing_size=2000,\n",
    "                                        verbose=True)\n",
    "\n",
    "# Create user-by-item matrix - this will keep track of order of users and movies in u and v\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Read in user and movie matrices\n",
    "user_file = open(\"user_matrix\", 'rb')\n",
    "user_mat = pickle.load(user_file)\n",
    "user_file.close()\n",
    "\n",
    "movie_file = open(\"movie_matrix\", 'rb')\n",
    "movie_mat = pickle.load(movie_file)\n",
    "movie_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4af38",
   "metadata": {},
   "source": [
    "### Validating Predictions\n",
    "\n",
    "Unfortunately, you weren't able to make predictions on every user-movie combination in the test set, as some of these users or movies were new.  \n",
    "\n",
    "However, you can validate your predictions for the user-movie pairs that do exist in the user_mat and movie_mat matrices.  \n",
    "\n",
    "`1.` Complete the function below to see how far off we were on average across all of the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf66083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_validation_comparison(val_df, \n",
    "                             user_mat=user_mat, \n",
    "                             movie_mat=movie_mat,\n",
    "                             verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - val_df (mandatory) - the validation dataset created in the third cell \n",
    "        above\n",
    "      - user_mat (mandatory) - U matrix in FunkSVD\n",
    "      - movie_mat (mandatory) - V matrix in FunkSVD\n",
    "    Outputs:\n",
    "      - rmse - RMSE of how far off each value is from it's predicted value\n",
    "      - perc_rated - percent of predictions out of all possible that could be rated\n",
    "      - actual_v_pred - a 10 x 10 grid with counts for actual vs predicted values\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    sse = 0\n",
    "    num_rated = 0\n",
    "    preds, acts = [], []\n",
    "    actual_v_pred = np.zeros((10,10))\n",
    "    for idx in range(len(val_users)):\n",
    "        try:\n",
    "            pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "            sse += (val_ratings[idx] - pred)**2\n",
    "            num_rated+=1\n",
    "            preds.append(pred)\n",
    "            acts.append(val_ratings[idx])\n",
    "            actual_v_pred[11-int(val_ratings[idx]-1), int(round(pred)-1)]+=1\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rmse = np.sqrt(sse/num_rated)\n",
    "    perc_rated = num_rated/len(val_users)\n",
    "    \n",
    "    output = rmse, perc_rated, actual_v_pred, preds, acts\n",
    "    \n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43477706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did we do?\n",
    "rmse, perc_rated, actual_v_pred, preds, acts = fn_validation_comparison(val_df=val_df,\n",
    "                                                                        verbose=True)\n",
    "print(rmse, perc_rated)\n",
    "sns.heatmap(actual_v_pred);\n",
    "plt.xticks(np.arange(10), np.arange(1,11));\n",
    "plt.yticks(np.arange(10), np.arange(1,11));\n",
    "plt.xlabel(\"Predicted Values\");\n",
    "plt.ylabel(\"Actual Values\");\n",
    "plt.title(\"Actual vs. Predicted Values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(acts, normed=True, alpha=.5, label='actual');\n",
    "plt.hist(preds, normed=True, alpha=.5, label='predicted');\n",
    "plt.legend(loc=2, prop={'size': 15});\n",
    "plt.xlabel('Rating');\n",
    "plt.title('Predicted vs. Actual Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c96b58",
   "metadata": {},
   "source": [
    "`2.` We didn't do so bad on making those predictions!  But, how many user-movie pairs were we unable to make predictions for?  Use the cell below to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbda61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above, this can be calculated as follows:\n",
    "print(\"Number not rated {}\".format(int(len(val_df['rating'])*(1-perc_rated))))\n",
    "print(\"Number rated {}.\".format(int(len(val_df['rating'])*perc_rated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27214ed2",
   "metadata": {},
   "source": [
    "### Content Based For New Movies\n",
    "\n",
    "If all of the above went well, you will notice we still have work to do!  We need to bring in a few things we picked up from the last lesson to use for those new users and movies.  Below is the code used to make the content based recommendations, which found movies that were similar to one another.  This was from **5_Content_Based_Recommendations** in the previous lesson.\n",
    "\n",
    "The below function **find_similar_movies** will provide similar movies to any movie based only on content.  \n",
    "\n",
    "Run the cell below to gain access to the content based similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movies.iloc[:,4:])\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_find_similar_movie(movie_id,\n",
    "                          verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - movie_id (mandatory) - a movie_id\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - similar_movie - an array of the most similar movies by title\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    # find the row of each movie id\n",
    "    movie_idx = np.where(movies['movie_id'] == movie_id)[0][0]\n",
    "    \n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "    similar_idxs = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0]\n",
    "    \n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(movies.iloc[similar_idxs, ]['movie'])\n",
    "    \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "    \n",
    "    return similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_get_movie_name(movie_id,\n",
    "                       verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - movie_id (mandatory) - a list of movie ids\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - movies - a list of movie names associated with the movie_ids\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function get movies names started')\n",
    "    begin = time()\n",
    "\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "    \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "   \n",
    "    return movie_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916c962",
   "metadata": {},
   "source": [
    "### Rank Based For New Users\n",
    "\n",
    "From the above two code cells, we have a way to make recommendations for movie-user pairs that have ratings in any part of our user-movie matrix.  We also have a way to make ratings for movies that have never received a rating using movie similarities.\n",
    "\n",
    "In this last part here, we need a way to make recommendations to new users.  For this, our functions from **2_Most_Popular_Recommendations** in Lesson 1 will come in handy.  Run the cell below to have these functions available.\n",
    "\n",
    "Run the cell below to gain access to the rank based functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_create_ranked_df(movie,\n",
    "                        review,\n",
    "                        verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - movie (mandatory) - the movies dataframe\n",
    "      - review (mandatory) - the reviews dataframe\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "    ranked_movies - a dataframe with movies that are sorted by highest avg rating, more reviews, \n",
    "                    then time, and must have more than 4 ratings\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    # Pull the average ratings and number of ratings for each movie\n",
    "    movie_ratings = reviews.groupby('movie_id')['rating']\n",
    "    avg_ratings = movie_ratings.mean()\n",
    "    num_ratings = movie_ratings.count()\n",
    "    last_rating = pd.DataFrame(reviews.groupby('movie_id').max()['date'])\n",
    "    last_rating.columns = ['last_rating']\n",
    "\n",
    "    # Add Dates\n",
    "    rating_count_df = pd.DataFrame({'avg_rating': avg_ratings, 'num_ratings': num_ratings})\n",
    "    rating_count_df = rating_count_df.join(last_rating)\n",
    "\n",
    "    # merge with the movies dataset\n",
    "    movie_recs = movies.set_index('movie_id').join(rating_count_df)\n",
    "\n",
    "    # sort by top avg rating and number of ratings\n",
    "    ranked_movies = movie_recs.sort_values(['avg_rating', 'num_ratings', 'last_rating'], ascending=False)\n",
    "\n",
    "    # for edge cases - subset the movie list to those with only 5 or more reviews\n",
    "    ranked_movies = ranked_movies[ranked_movies['num_ratings'] > 4]\n",
    "        \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "            \n",
    "    return ranked_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_popular_recommendation(user_id, \n",
    "                              num_top, \n",
    "                              ranked_movie,\n",
    "                              verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - user_id (mandatory) - the user_id (str) of the individual you are \n",
    "        making recommendations for\n",
    "      - num_top (mandatory) - an integer of the number recommendations you want \n",
    "        back\n",
    "      - ranked_movie (mandatory) - a pandas dataframe of the already ranked \n",
    "        movies based on avg rating, count, and time\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - top_movie - a list of the n top recommended movies by movie title in order \n",
    "        best to worst\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function popular recommendations started')\n",
    "    begin = time()\n",
    "\n",
    "    top_movie = list(ranked_movie['movie'][:n_top])\n",
    "    \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "\n",
    "    return top_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1409a",
   "metadata": {},
   "source": [
    "### Now For Your Task\n",
    "\n",
    "The above cells set up everything we need to use to make predictions.  Your task is to write a function, which uses the above information as necessary to provide recommendations for every user in the **val_df** dataframe.  There isn't one right way to do this, but using a blend between the three could be your best bet.  \n",
    "\n",
    "You can see the blended approach I used in the video on the next page, but feel free to be creative with your solution!\n",
    "\n",
    "`3.` Use the function below along with the document strings to assist with completing the task for this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e22912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########1#########2#########3#########4#########5#########6#########7#########8\n",
    "def fn_make_recommendations(_id, \n",
    "                            _id_type='movie', \n",
    "                            train_data=train_data_df, \n",
    "                            train_df=train_df, \n",
    "                            movies=movies, \n",
    "                            rec_num=5, \n",
    "                            user_mat=user_mat,\n",
    "                            verbose=False):\n",
    "    '''This function...\n",
    "    \n",
    "    Source: Udacity Data Science Course - Lesson 7  - Matrix Factorization for\n",
    "    Recommendations - Forth Notebook - Class 20 - Cold Start Problem w/ FunkSVD\n",
    "    \n",
    "    Inputs:\n",
    "      - _id (mandatory) - either a user or movie id (int)\n",
    "      - _id_type (mandatory) - \"movie\" or \"user\" (str)\n",
    "      - train_data (mandatory)  - dataframe of data as user-movie matrix\n",
    "      - train_df (mandatory) - dataframe of training data reviews\n",
    "      - movies (mandatory) - movies df\n",
    "      - rec_num (optional) - number of recommendations to return (int)\n",
    "      - user_mat (optional) - the U matrix of matrix factorization\n",
    "      - movie_mat (optional) - the V matrix of matrix factorization\n",
    "      - verbose (optional) - if you want some verbosity in your function -\n",
    "        (Boolean, default=False)\n",
    "    Output:\n",
    "      - recs - (array) a list or numpy array of recommended movies like the \n",
    "        given movie, or recs for a user_id given\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('###function movies watched started')\n",
    "    begin = time()\n",
    "\n",
    "    # if the user is available from the matrix factorization data, \n",
    "    # I will use this and rank movies based on the predicted values\n",
    "    # For use with user indexing\n",
    "    val_user = train_data_df.index\n",
    "    rec_id = fn_create_ranked_df(movie, \n",
    "                                 train_df,\n",
    "                                 verbose=verbose)\n",
    "    \n",
    "    if _id_type == 'user':\n",
    "        if _id in train_data.index:\n",
    "            # Get the index of which row the user is in for use in U matrix\n",
    "            idx = np.where(val_user == _id)[0][0]\n",
    "            \n",
    "            # take the dot product of that row and the V matrix\n",
    "            pred = np.dot(user_mat[idx,:],movie_mat)\n",
    "            \n",
    "            # pull the top movies according to the prediction\n",
    "            indice = preds.argsort()[-rec_num:][::-1] #indices\n",
    "            rec_id = train_data_df.columns[indices]\n",
    "            rec_name = fn_get_movie_name(rec_id,\n",
    "                                         verbose=verbose)         \n",
    "        else:\n",
    "            # if we don't have this user, give just top ratings back\n",
    "            rec_names = fn_popular_recommendation(_id, \n",
    "                                                  rec_num, \n",
    "                                                  ranked_movie,\n",
    "                                                  verbose=verbose)\n",
    "            \n",
    "    # Find similar movies if it is a movie that is passed\n",
    "    else:\n",
    "        rec_id = fn_find_similar_movie(_id,\n",
    "                                       verbose=verbose)\n",
    "        rec_name = get_movie_name(rec_id,\n",
    "                                  verbose=verbose)\n",
    "        \n",
    "    output = rec_id, rec_name\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    if verbose:\n",
    "        print('elapsed time: {}s'.format(end-begin))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_make_recommendation(_id=48, \n",
    "                       _id_type='user'\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(val_df['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6458b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendations\n",
    "user_rec_dict_with_top = dict()\n",
    "for user_id in set(val_df['user_id']):\n",
    "    user_rec_dict_with_top[user_id] = fn_make_recommendation(_id=user_id, \n",
    "                                                             _id_type='user')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnter = 0\n",
    "for user, rec in user_rec_dict_with_top.items():\n",
    "    if cnter < 12:\n",
    "        print(\"For user {}, our recommendations are: \\n {}\".format(user, rec))\n",
    "        cnter+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d5a8d",
   "metadata": {},
   "source": [
    "**This recommendation style looks like it may do okay with accuracy, but it seems like a lot of the same movies are showing up.  When we think back to serendipity, novelty, and diversity as means of a good recommendation system, this set of recommendations still isn't great.  We might consider providing some content based recommendations from movies an individual has watched along with these recommendations to meet those categories of a good recommender.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
